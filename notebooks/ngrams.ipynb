{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Modeling\n",
    "\n",
    "- Language Understanding Systems\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "- Get the dataset for exercises from https://github.com/esrel/NL2SparQL4NLU .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NL2SparQL4NLU already exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -d \"NL2SparQL4NLU\" ]\n",
    "then\n",
    "    git clone https://github.com/esrel/NL2SparQL4NLU.git\n",
    "else\n",
    "    echo \"NL2SparQL4NLU already exists\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research.\n",
    "\n",
    "__Corpus properties__:\n",
    "- Format\n",
    "- Language\n",
    "- Annotation\n",
    "- Split for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who plays luke on star wars new hope\n",
      "show credits for the godfather\n",
      "who was the main actor in the exorcist\n",
      "find the female actress from the movie she 's the man\n",
      "who played dory on finding nemo\n",
      "who was the female lead in resident evil\n",
      "who played guido in life is beautiful\n",
      "who was the co-star in shoot to kill\n",
      "find the guy who plays charlie on charlie 's angels\n",
      "cast and crew of movie the campaign\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "trn=NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt\n",
    "tst=NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt\n",
    "\n",
    "cat $trn | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Format__:\n",
    "\n",
    "- Utterance (sentence) per line\n",
    "- Tokenized\n",
    "- Lowercased\n",
    "\n",
    "__Language__: English monolingual\n",
    "\n",
    "__Annotation__: None (for now)\n",
    "\n",
    "__Split__: training & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Counting\n",
    "\n",
    "*Corpus* description in terms of:\n",
    "\n",
    "- total number of words\n",
    "- total number of utterances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "##### Objective\n",
    "\n",
    "Compute the corpus descriptive statistics above (total utterance and word counts) for the __training__ and __test__ sets of NL2SparQL4NLU dataset. Compare the computed statistics with the reference values below.\n",
    "\n",
    "\n",
    "| Metric           | Train  | Test   |\n",
    "|------------------|-------:|-------:|\n",
    "| Total Words      | 21,453 |  7,117 |\n",
    "| Total Utterances |  3,338 |  1,084 |\n",
    "\n",
    "\n",
    "##### Bash Solution\n",
    "\n",
    "```bash\n",
    "wc -lw $trn\n",
    "wc -lw $tst\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "\n",
    "##### Objective\n",
    "Compute *lexicon size* (total number of unique words) of the NL2SparQL4NLU dataset and compare to the reference values.\n",
    "\n",
    "| Metric       | Value |\n",
    "|--------------|------:|\n",
    "| Lexicon Size | 1,729 |\n",
    "\n",
    "##### Algorithm\n",
    "\n",
    "1. Tokenize as token-per-line\n",
    "2. Sort\n",
    "3. Remove duplicates\n",
    "\n",
    "##### Bash Solution\n",
    "\n",
    "```bash\n",
    "cat $trn |\\\n",
    "    tr ' ' '\\n' |\\  # convert to token-per-line (replace spaces with newline)\n",
    "    sort |\\         # sort (alphabetically)\n",
    "    uniq |\\         # remove duplicates (unique)\n",
    "    wc -l           # count lines\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Frequency Lists\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts (our corpus is lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "##### Objective\n",
    "Compute frequencies of words in the lexicon (using only training set). Compare the 5 most frequent words to the table below.\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| the    |     1,337 |\n",
    "| movies |     1,126 |\n",
    "| of     |       607 |\n",
    "| in     |       582 |\n",
    "| movie  |       564 |\n",
    "\n",
    "##### Algorithm\n",
    "\n",
    "1. Tokenize as token-per-line\n",
    "2. Sort\n",
    "3. Count occurences of each word\n",
    "\n",
    "##### Bash Solution\n",
    "\n",
    "```bash\n",
    "cat $trn |\\\n",
    "    tr ' ' '\\n' |\\                    # convert to token-per-line (replace spaces with newline)\n",
    "    sort |\\                           # sort (alphabetically)\n",
    "    uniq -c |\\                        # remove duplicates (unique) and count occurences\n",
    "    sort -gr |\\                       # reverse sort the list by frequency\n",
    "    awk '{OFS=\"\\t\";print $2, $1}' |\\  # swap output columns [optional]\n",
    "    head -n 5                         # get top 5 lines\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). \n",
    "\n",
    "__Frequency Cut-Off__: \n",
    "\n",
    "- remove least frequent words (with respect to frequency threshold)\n",
    "- remove most frequent words (with respect to frequency threshold)\n",
    "- remove *stop words*\n",
    "\n",
    "#### Stop Words\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Compare Frequency List to the stop word list in `NL2SparQL4NLU/extras/english.stop.txt`.\n",
    "\n",
    "##### Objective\n",
    "Create lexicons applying the following steps and report lexicon sizes. Compare to the reference values in the table.\n",
    "\n",
    "- Remove words that occure less than 2 times, i.e. remove words with frequency 1 ([hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "- Remove words that occure more that 100 times\n",
    "- Remove stop words from the lexicon\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| cut-off    |   2 | N/A |  950 |\n",
    "| cut-off    | N/A | 100 | 1694 |\n",
    "| cut-off    |   2 | 100 |  915 |\n",
    "| stop words | N/A | N/A | 1529 |\n",
    "\n",
    "##### Algorithm(s)\n",
    "\n",
    "__Cut-off__\n",
    "1. Compute Frequency List\n",
    "2. Remove words below/above the threshold\n",
    "\n",
    "__Stop Word Removal__\n",
    "1. Compute Lexicon\n",
    "2. Remove words that also appear in stop word list\n",
    "\n",
    "##### Bash Solution\n",
    "\n",
    "- SLOW! (convert cell to 'code' & replace \"\\`\\`\\`bash\" with \"`%%bash`\", & delete final \"\\`\\`\\`\")\n",
    "- Use PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "trn=NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt\n",
    "swl=NL2SparQL4NLU/extras/english.stop.txt\n",
    "\n",
    "# create frequency list\n",
    "cat $trn | tr ' ' '\\n' | sort | uniq -c | awk '{OFS=\"\\t\";print $2, $1}' > lexicon.txt\n",
    "\n",
    "function cutoff() {\n",
    "    # use -1 for th_min and th_max to ignore    \n",
    "    freq_lex=$1\n",
    "    th_min=$2\n",
    "    th_max=$3\n",
    "    sw_file=$4\n",
    "    \n",
    "    local df_min=1\n",
    "    local df_max=$((2**32))\n",
    "    \n",
    "    th_min=$(( th_min != -1 ? th_min : df_min ))\n",
    "    th_max=$(( th_max != -1 ? th_max : df_max ))\n",
    "    \n",
    "    declare -a stopwords\n",
    "    declare -a lexicon\n",
    "    \n",
    "    if [[ $sw_file != '' ]]\n",
    "    then\n",
    "        IFS=$'\\n' read -d '' -r -a stopwords < $sw_file        \n",
    "    fi\n",
    "    \n",
    "    swcount=0\n",
    "    while read -r token count\n",
    "    do\n",
    "        if (( count >= th_min && count <= th_max ))\n",
    "        then\n",
    "            if printf '%s\\n' ${stopwords[@]} | grep -q \"^${token}$\"\n",
    "            then\n",
    "                ((swcount++))\n",
    "            else\n",
    "                lexicon+=( $token )\n",
    "            fi     \n",
    "        fi\n",
    "    done < ${freq_lex}\n",
    "    \n",
    "    # print size of lexicon\n",
    "    echo ${#lexicon[@]}\n",
    "}\n",
    "\n",
    "cutoff lexicon.txt -1 -1\n",
    "cutoff lexicon.txt 2 -1\n",
    "cutoff lexicon.txt -1 100\n",
    "cutoff lexicon.txt 2 100\n",
    "cutoff lexicon.txt -1 -1 $swl\n",
    "cutoff lexicon.txt 2 100 $swl\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ngrams and Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of *n* items from a given sequence of text or speech. An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character n-grams: cat\n",
    "- word n-grams: the cat is fat\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['c', 'a', 't']` | `['ca', 'at']` | `['cat']` |\n",
    "| *Word N-grams*      | `['the', 'cat', 'is' , 'fat']` | `['the cat', 'cat is', ...]` | `['the cat is', ...]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Counting Bigrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count. Bigram count only differs in a unit of counting -- pairs of words. \n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "##### Objective\n",
    "Compute bigram frequencies in the training set of NL2SparQL4NLU corpus. Compate top 5 most frequent bigrams to the values in table.\n",
    "\n",
    " word 1 | word 2 | count \n",
    ":-------|:-------|-------:\n",
    "show    | me     |   377\n",
    "the     | movie  |   267\n",
    "of      | the    |   186\n",
    "me      | the    |   122\n",
    "is      | the    |   120\n",
    "\n",
    "##### Algorithm (for bash)\n",
    "1. tokenize as token-per-line\n",
    "2. print word#1 and word#2 side by side\n",
    "3. count\n",
    "\n",
    "##### Bash Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show\tme\t377\n",
      "the\tmovie\t267\n",
      "of\tthe\t186\n",
      "me\tthe\t122\n",
      "is\tthe\t120\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "trn=NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt\n",
    "\n",
    "cat $trn | tr ' ' '\\n' > w1.txt\n",
    "tail -n +2 w1.txt > w2.txt\n",
    "paste w1.txt w2.txt > bigrams.txt\n",
    "cat bigrams.txt | sort | uniq -c | sort -gr |\\\n",
    "    awk '{OFS=\"\\t\";print $2, $3, $1}' |\\\n",
    "    head -n 5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Counting Trigrams\n",
    "\n",
    "#### Exercise:\n",
    "Extend bigram counting to trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Improving N-grams: Sentence beginning & end tags\n",
    "\n",
    "The bash bigram counting above has an issue of counting n-grams across sentence boundaries. Including sentence boundary markers leads to a better model.\n",
    "\n",
    "#### Exercise:\n",
    "Add sentence beginning (`<S>`) and sentence end (`</S>`) tags and recompute bigrams. (Remove `</S> <S>` from counts. Also take care of runover unigrams.)\n",
    "\n",
    "##### Bash Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S>\twhat\t511\n",
      "<S>\tshow\t450\n",
      "show\tme\t377\n",
      "movies\t</S>\t333\n",
      "<S>\tfind\t268\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "trn=NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt\n",
    "\n",
    "cat $trn | sed 's/^/<S> /g;s/$/ <\\/S>/g' | tr ' ' '\\n' > w1.txt\n",
    "tail -n +2 w1.txt > w2.txt\n",
    "paste w1.txt w2.txt | sed '/<\\/S>\t<S>/d' > bigrams.txt\n",
    "cat bigrams.txt | sort | uniq -c | sort -gr |\\\n",
    "    awk '{OFS=\"\\t\";print $2, $3, $1}' |\\\n",
    "    head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Computing N-gram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1. Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of n-grams can be computed from relative frequency counts (*Maximum Likelihood Estimation*), as follows.\n",
    "\n",
    "N-gram   | Equation                      |\n",
    ":--------|:------------------------------|\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ |\n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$\n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurences of $x$ in a corpus (x could be unigram, bigram, etc.)\n",
    "\n",
    "#### Exercise(s):\n",
    "\n",
    "- Compute (automatically) probabilities of each token in the lexicon.\n",
    "- Calculate N-gram probabilities of:\n",
    "  \n",
    "  - $p(the | of)$\n",
    "  - $p(the | is)$\n",
    "  - $p(play | the)$\n",
    "  - all n-gram probabilities of words after `italy', i.e. $p(*|italy)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_{1},\\ldots ,w_{m})$ to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute a language model of our corpus.\n",
    "\n",
    "It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Computing Probability of a Sequence (String)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Probability of a Sequence: [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities (chain rule). \n",
    "\n",
    "$$p(w_1,...,w_T)=p(w_1) \\prod_{i=2}^T p(w_i|w_1,...,w_{i-1}) = p(w_1) \\prod^T_{i=2} p(w_i|h_i)$$\n",
    "\n",
    "Where $h_i = \\{w_1,...,w_{i-1}\\}$ is a history (previous context). The order of n-gram, truncates the history to length $n - 1$. Making a simplifying assumption that probability of a curent element only depends on previous $n$ elements.\n",
    "\n",
    "$$p(w_i|w_1,...,w_{i-1}) = p(w_i|w_{i-n+1},...,w_{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\n",
    "N-gram   | Equation                   |\n",
    ":--------|:---------------------------|\n",
    "unigram  | $$p(w_i)$$                 |\n",
    "bigram   | $$p(w_i|w_{i-1})$$         |\n",
    "trigram  | $$p(w_i|w_{i-2},w_{i-1})$$ |\n",
    "\n",
    "__Example__: \"The cat is fat\"\n",
    "\n",
    "$$p(\\langle s \\rangle, the, cat, is, fat, \\langle /s \\rangle) =$$\n",
    "$$= p(the | \\langle s \\rangle) * p(cat | the) * p(is | cat) * p(fat | is) * p(\\langle /s \\rangle | fat) = $$\n",
    "$$= 0.25 * 0.10 * 0.20 * 0.05 * 0.15 = 0.0000375$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Using computed n-gram probabilities compute the probabilities of utterances in `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`\n",
    "\n",
    "__Expect Errors!__ (we will address them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Language Model as Automaton\n",
    "\n",
    "Language Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = \\langle s \\rangle$;\n",
    "- *while* $w_i \\neq \\langle /s \\rangle$\n",
    "\n",
    "    - stochastically get new word w.r.t. $p(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Using computed n-gram probabilities implement a function to generate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. LM Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Underflow Problem\n",
    "\n",
    "Probabilities are usually small ($<1$).\n",
    "Multiplying many of those may cause *underflow* problem.\n",
    "\n",
    "##### Solution\n",
    "Use the sum of the probabilities' logs instead of product\n",
    "\n",
    "| Properties     |\n",
    "|:---------------|\n",
    "| $$p(a) > p(b)$$\n",
    "| $$log(p(a)) > log(p(b))$$\n",
    "| $$log(a*b) = log(a) + log(b)$$\n",
    "| $$p(a) * p(b) \\rightarrow log(p(a)) + log(p(b))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Data Sparseness\n",
    "\n",
    "Many rare, but possible combinations are not present in training data. \n",
    "Unseen words and n-grams have $0$ probability. \n",
    "Thus, whole sequence gets $0$ probability.\n",
    "\n",
    "__Example__: Unseen Word: \"cow\"\n",
    "\n",
    "$$p(\\langle s \\rangle, the, cow, is, fat, \\langle /s \\rangle) =$$ \n",
    "$$= p(the | \\langle s \\rangle) * p(cow | the) * p(is | cow) * p(fat | is) * p(\\langle /s \\rangle | fat) = $$\n",
    "$$= 0.25 * 0.00 * 0.00 * 0.05 * 0.15 = 0.00$$\n",
    "\n",
    "The problem is somewhat avoided using log probabilities.\n",
    "\n",
    "##### Solution: Smoothing\n",
    "- Add some probability to unseen events\n",
    "- Remove some probability from seen events -- __discounting__\n",
    "- Joint probability distribution sums to 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. Out-Of-Vocabulary (OOV) Rate\n",
    "\n",
    "*OOV Rate*: % of word tokens in test data that are not contained in the lexicon (vocabulary).\n",
    "Empirically each OOV word results in 1.5 - 2 extra errors (> 1 due to the loss of contextual information).\n",
    "\n",
    "#### Exercise (Optional):\n",
    "Compute OOV Rate for `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Handling Unseen Words\n",
    "\n",
    "How to handle words (in test set) that were never seen in the training data?\n",
    "Train a language model with specific token (e.g. `<UNK>`) for unknown words!\n",
    "How to estimate probabilities of unknown words and n-grams?\n",
    "\n",
    "The *simplest* approach is to replace all the words that are not in vocabulary (lexicon) with the `<UNK>` token and treat it as any other word. (For instance, applying frequency cut-off to the lexicon, will allow estimate these probabilities on the training set.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Smoothing\n",
    "\n",
    "Available smoothing methods: ([tutorial](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf))\n",
    "- [Additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (__simplest__)\n",
    "- Good-Turing estimate\n",
    "- Jelinek-Mercer smoothing (interpolation)\n",
    "- Katz smoothing (backoff)\n",
    "- Witten-Bell smoothing\n",
    "- Absolute discounting\n",
    "- Kneser-Ney smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1. Add-One Smoothing\n",
    "Kind of Additive Smoothing, imaginary training data where all possible n-gram combinations occur once.\n",
    "\n",
    "__Bigrams__\n",
    "\n",
    "$V$ -- bigram vocabulary size\n",
    "\n",
    "$$p(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$\n",
    "\n",
    "__N-grams__\n",
    "\n",
    "$V$ -- total number of possible $(N-1)$-grams\n",
    "\n",
    "$$p(w_i | w^{i-1}_{i-N+1}) = \\frac{c(w^{i-1}_{i-N+1},w_i)+1}{c(w^{i-1}_{i-N+1})+V}$$\n",
    "\n",
    "Typically, we assume $V = \\{w : c(w) > 0\\} \\cup \\{<UNK>\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Exercise: Putting it all together\n",
    "\n",
    "Train a Language Model (compute n-gram probabilities) such that:\n",
    "\n",
    "- case insensitive (by default)\n",
    "- 2-gram\n",
    "- log probabilities\n",
    "- considers sentence boundaries (beginning and end of sentence tags)\n",
    "- considers unknown words\n",
    "- Add-One Smoothing\n",
    "\n",
    "Compute probabilties of utterances in `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. LM Evaluation: [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data;\n",
    "- Probability of test data;\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\n",
    "$$ PPL = \\sqrt[N]{\\frac{1}{p(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}p(w_i|w_{i-N+1})}}$$\n",
    "\n",
    "Where $N$ is the number of words in test set;\n",
    "\n",
    "\n",
    "#### Exercise (Optional):\n",
    "Calculate Perplexity for `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
