{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling with Weighted Finite State Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language Understanding Systems Lab\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the Laboratory Work for [Language Understanding Systems class](http://disi.unitn.it/~riccardi/page7/page13/page13.html) of [University of Trento](https://www.unitn.it/en).\n",
    "Laboratory has been ported to jupyer notebook format for remote teaching during [COVID-19 pandemic](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [OpenFST](http://www.openfst.org/twiki/bin/view/FST/WebHome)\n",
    "- [OpenGRM](http://www.opengrm.org/twiki/bin/view/GRM/NGramLibrary)\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Shallow Parsing with WFSMs: Natural Language Understanding\n",
    "\n",
    "Language Understanding of several tasks, one of which is entity extraction (concept tagging). The task is usually approached as Shallow Parsing, where we segment the input into constituents and label them using IOB-schemes.\n",
    "\n",
    "Using Weighted Finite State Machines for the task provides several benefits:\n",
    "- Even though we saw how to do sequence labeling using HMM, in real applications models can become quite complex to solve. \n",
    "- The task usually involves several components (e.g. *emission* & *transition* probabilities), and WFSMs provide an efficient way to represent and process this components via intersection and composition operations.\n",
    "    - WFSTs are good at modeling HMM and solving state machine problems\n",
    "    - Weights can be associated with edges as costs or probabilities (default: cost = negative log probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Common Sequence Labeling Pipeline\n",
    "\n",
    "The common approach to concept tagging (or sequence labeling in general) makes use of 3 components:\n",
    "\n",
    "|                   | Description                      \n",
    "|:------------------|:------------------------------\n",
    "| $$\\lambda_{W}$$   | FSA representation of an input sentence\n",
    "| $$\\lambda_{W2T}$$ | FST to translate words into output labels (e.g. `iob+type`)\n",
    "| $$\\lambda_{*LM}$$ | FSA Ngram Language Model to score the sequences of output labels\n",
    "\n",
    "Consequently, Sequence Labeling ($\\lambda$) is performed by composition of these three components as:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T} \\circ \\lambda_{*LM}$$\n",
    "\n",
    "- It is common to include other components to perform intermediate operations for:\n",
    "    - generalization of input ($\\lambda_{G}$)\n",
    "    - cleaning of output\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. General Setup\n",
    "Let's start by preparing our workspace.\n",
    "\n",
    "What we have is:\n",
    "- training & test sets in utterance-per-line format\n",
    "- training & test sets in CoNLL format that contain word-tag observations\n",
    "\n",
    "For working with WFSMs we need:\n",
    "- input symbol table: words\n",
    "- output symbol table: tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Preprocessing:\n",
    "To handle OOV (unknown) words let's:\n",
    "- apply frequency cut-off to lexicon\n",
    "- replace OOV words in both training and test data with `<unk>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOV with Frequency Cut-off using OpenGRM NGram Library Tools\n",
    "\n",
    "It is easy to apply frequency cut-off and replace OOV with `<unk>` using other means (e.g. `python`). \n",
    "Here we demonstrate how to achieve that using provided tools and some `unix` commands.\n",
    "\n",
    "__Objective__: map low frequency unigrams to `<unk>`\n",
    "\n",
    "- generate symbol table using `ngramsymbols` for a corpus\n",
    "- compile the corpus into FAR using `farcompilestrings`\n",
    "- count unigrams in FAR using `ngramcount`\n",
    "- print counts using `ngramprint`\n",
    "- filter the words externally and save them into a file\n",
    "- generate a new symbol table using `ngramsymbols`\n",
    "- recompile the corpus into a new FAR using the new symbol table and `farcompilestrings`\n",
    "\n",
    "> __Note__: *if you provide an external word list to `ngramsymbols` it will generate a symbol table in the required format.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we will be using corpus files a lot, let's copy them into current directory with shorter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dpath='NL2SparQL4NLU/dataset/NL2SparQL4NLU'\n",
    "\n",
    "cp $dpath.train.utterances.txt trn.txt\n",
    "cp $dpath.test.utterances.txt tst.txt\n",
    "\n",
    "cp $dpath.train.conll.txt trn.conll\n",
    "cp $dpath.test.conll.txt tst.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling OOV with OpenFST and OpenGRM tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create full symbol table\n",
    "ngramsymbols trn.txt trn.isyms.tmp\n",
    "# compile into FAR\n",
    "farcompilestrings --symbols=trn.isyms.tmp --keep_symbols trn.txt trn.far.tmp\n",
    "# count unigrams\n",
    "ngramcount --order=1 trn.far.tmp trn.cnt.tmp\n",
    "# print counts as integers\n",
    "ngramprint --integers trn.cnt.tmp trn.cnt.txt.tmp\n",
    "\n",
    "# bash: you can use python to process file\n",
    "while read -r word freq; do \\\n",
    "    if (( freq > 1 )); then echo $word ; fi\n",
    "done < trn.cnt.txt.tmp > trn.cnt.txt.cutoff.tmp\n",
    "\n",
    "# final input symbol table\n",
    "ngramsymbols trn.cnt.txt.cutoff.tmp isyms.txt\n",
    "\n",
    "# delete temp files\n",
    "rm -f *.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# let's compile both training and test set into far using this symbol table\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.txt trn.far\n",
    "\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    tst.txt tst.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have:\n",
    "- Symbol table (`isyms.txt`)\n",
    "    - contains `['<s>', '</s>', '<epsilon>', '<unk>']` that are added automatically\n",
    "- Training data as FAR with OOV replaced (`trn.far`)\n",
    "- Test data as FAR with OOV replaced (`tst.far`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating Output Symbol Table\n",
    "To do sequence labeling we additionally require *output symbol table*.\n",
    "\n",
    "In case we know the concepts (__types__), we can build our output symbol table without looking at data.\n",
    "\n",
    "Since our output labels are composed of segmentation (`iob`) and classification (`type`) labels, we can make sure that each __type__ has all possible IOB prefixes.\n",
    "\n",
    "- prefix each __type__ with all possible IOB prefixes (i.e. `I-` and `B-`)\n",
    "- generate symbol table using `ngramsymbols` (as shown above)\n",
    "\n",
    "In case we don't know the list of __types__, we have to extract __types__ from training data in CoNLL format (stripping `iob` prefix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create a unique list of types\n",
    "cat trn.conll | cut -f 2 | cut -d '-' -f 2 | sed '/^ *$/d' | sort | uniq > types.txt\n",
    "\n",
    "# prefix each type with segmentation information\n",
    "while read -r word\n",
    "do\n",
    "    if [[ $word != 'O' ]]\n",
    "    then\n",
    "        echo \"B-$word\"\n",
    "        echo \"I-$word\"\n",
    "    else\n",
    "        echo $word\n",
    "    fi\n",
    "done < types.txt > osyms.tmp\n",
    "\n",
    "# generate output symbol table with iob-prefixed typed\n",
    "ngramsymbols osyms.tmp osyms.txt\n",
    "\n",
    "rm -f *.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Applying FSTs\n",
    "- Our objective is to be able to annotate input sentences (as FSAs in FAR) using the machines we are going to build. \n",
    "- Our test data has been loaded into FAR archive\n",
    "- We will can `farextract` to extract these sentence FSAs and apply our FSMs to them\n",
    "    - `farextract --filename_prefix=\"<odir>\" <FAR>` will extract contents of `<FAR>` into directory `<odir>`\n",
    "    - we can iterate over files in the directory and apply operations (see below)\n",
    "- We can also create FAR of the processed FSMs using `farcreate` as\n",
    "    - `farcreate --file_list_input <list of FST filenames> <output FAR>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tstar\n",
      "1\t2\tof\tof\n",
      "2\t3\t<unk>\t<unk>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "mkdir -p $wdir\n",
    "\n",
    "farextract --filename_prefix=\"$wdir/\" tst.far\n",
    "cp $wdir/tst.txt-0001 sent.fsa\n",
    "\n",
    "fstprint sent.fsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For testing we are going to use this `sent.fsa`\n",
    "- For evaluation we are going to iterate over whole FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Baselines\n",
    "Let's demonstrate the process by building some simple Shallow Parsing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Random\n",
    "The simplest solution is to assign output labels __randomly__. \n",
    "\n",
    "To achieve this we need to:\n",
    "- implement an FST that translates our words into output symbols ($\\lambda_{W2T}$) with equal cost, or no cost at all (i.e. unweighted FST)\n",
    "    - let's call it $\\lambda_{W2T_{U}}$ for [universe](https://en.wikipedia.org/wiki/Universe_(mathematics)).\n",
    "- compose it with our sentence\n",
    "- choose a random path in FST\n",
    "\n",
    "The FST $\\lambda_{W2T_{U}}$ represents search space for $p(w_i|t_i)$ without being exposed to any observation. It is build using only our knowledge of the __domain__:\n",
    "- vocabulary of language (input symbols)\n",
    "- concepts __types__ in our domain\n",
    "\n",
    "input and output symbols and all translations are possible. \n",
    "\n",
    "Since we have no model yet, the whole pipeline is:\n",
    "\n",
    "$$\\lambda_{R} = \\lambda_{W} \\circ \\lambda_{W2T_{U}}$$\n",
    "\n",
    "- __*random path*__ here is opposed to __*best path*__ or __*shortest path*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's define a function in python to write FST specification given input and output symbol tables as below\n",
    "    - we will be using it a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2t(isyms, osyms, sep='+', out='w2t.tmp'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'\n",
    "    state = '0'\n",
    "    fs = \" \"  # column sepataror for fst\n",
    "    \n",
    "    ist = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(isyms, 'r')]) - special))\n",
    "    ost = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(osyms, 'r')]) - special))\n",
    "    \n",
    "    with open(out, 'w') as f:\n",
    "        for i in range(len(ist)):\n",
    "            for j in range(len(ost)):\n",
    "                f.write(fs.join([state, state, ist[i], ost[j]]) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t('isyms.txt', 'osyms.txt', out='w2t_u.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         45648\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=osyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2t_u.txt w2t_u.bin\n",
    "\n",
    "fstinfo w2t_u.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Compute input and output symbol table sizes\n",
    "- Compare their multiplication to `# of arcs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "- note the usage of `fstrandgen` instead of `fstshortestpath` to get __random__ paths of FST.\n",
    "- All tokens will be predicted as `O`, if we use `fstshortestpath`.\n",
    "    - Bonus Question: *Why?* (try uncommenting & running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tI-person.name\n",
      "1\t2\tof\tB-movie.description\n",
      "2\t3\t<unk>\tI-movie.release_date\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_u.bin | fstrandgen | fstrmepsilon | fsttopsort | fstprint\n",
    "# fstcompose sent.fsa w2t_u.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "- For evaluation we are going to use `conlleval.pl` script (provided is `src` directory)\n",
    "- We need to convert output to the appropriate format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collecting prediction from our model & storing them into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t_u.bin | fstrandgen | fstrmepsilon | fsttopsort | fstprint\n",
    "done > w2t_u.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- post-process the output to have at least 3 columns, such that:\n",
    "    - column #1 (or 0) is words\n",
    "    - column #2 (last-1) is reference labels\n",
    "    - column #3 (last) is predicted labels\n",
    "    - replace tabs (`\\t`) with spaces (`' '`) and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 7117 tokens with 1091 phrases; found: 6822 phrases; correct: 20.\n",
      "accuracy:   2.15%; precision:   0.29%; recall:   1.83%; FB1:   0.51\n",
      "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  121\n",
      "       actor.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  284\n",
      "actor.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  292\n",
      "       actor.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  306\n",
      "   award.category: precision:   0.00%; recall:   0.00%; FB1:   0.00  326\n",
      "   award.ceremony: precision:   0.36%; recall:  14.29%; FB1:   0.71  274\n",
      "   character.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  308\n",
      "     country.name: precision:   1.75%; recall:   8.06%; FB1:   2.88  285\n",
      "    director.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  303\n",
      "director.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  275\n",
      "movie.description: precision:   0.00%; recall:   0.00%; FB1:   0.00  296\n",
      "      movie.genre: precision:   0.32%; recall:   2.78%; FB1:   0.57  314\n",
      "movie.gross_revenue: precision:   0.00%; recall:   0.00%; FB1:   0.00  266\n",
      "   movie.language: precision:   0.32%; recall:   1.45%; FB1:   0.53  310\n",
      "   movie.location: precision:   0.00%; recall:   0.00%; FB1:   0.00  289\n",
      "       movie.name: precision:   1.37%; recall:   0.85%; FB1:   1.05  291\n",
      "movie.release_date: precision:   0.00%; recall:   0.00%; FB1:   0.00  294\n",
      "movie.release_region: precision:   0.00%; recall:   0.00%; FB1:   0.00  271\n",
      "movie.star_rating: precision:   0.00%; recall:   0.00%; FB1:   0.00  279\n",
      "    movie.subject: precision:   0.00%; recall:   0.00%; FB1:   0.00  285\n",
      "       movie.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      person.name: precision:   0.72%; recall:   5.88%; FB1:   1.29  277\n",
      "person.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  274\n",
      "    producer.name: precision:   0.70%; recall:   2.74%; FB1:   1.11  287\n",
      "      rating.name: precision:   1.27%; recall:   6.56%; FB1:   2.13  315\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "paste tst.conll w2t_u.out | cut  -f 1,2,6 | tr '\\t' ' ' | sed 's/^ .*//g' > w2t_u.out.conll \n",
    "perl ../src/conlleval.pl < w2t_u.out.conll                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Output Symbol Priors\n",
    "\n",
    "The simplest form for the third component is to use output label priors, i.e. unigram probabilities of output labels. To model that, we can:\n",
    "- train a unigram language model using `ngramcount` & `ngrammake` (let's call it $\\lambda_{LM_{1}}$)\n",
    "- compose it with the $\\lambda_{W2T_{U}}$ so that the whole pipeline becomes:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "- Since `O` tag is the most frequent & we will have a model that always predicts it.\n",
    "- We can represent our model as:\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Considerations__\n",
    "- In this baseline we model __observations__. Due to the fact that:\n",
    "    - our models are based on observations\n",
    "    - OOV is due to scarcity of observations\n",
    "- We need to change output symbol table of $\\lambda_{W2T_{U}}$ to output only tags present in data or `<unk>`. \n",
    "    - it might happen such that an `iob+type` combination never appears in our training data\n",
    "    - output symbol tables of $\\lambda_{W2T}$ (FST) and symbol table of $\\lambda_{LM_{1}}$ (FSA) have to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Training\" a Model\n",
    "A new $\\lambda_{*LM}$ model is built following these steps:\n",
    "1. prepare training data for model (in required format)\n",
    "2. prepare symbol table for that data\n",
    "    - apply OOV handing (you can use any of the approaches to introduce `<unk>`)\n",
    "3. compile training data into FAR using this symbol table\n",
    "4. estimate model probabilities for $\\lambda_{*LM}$ (i.e. train ngram model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new $\\lambda_{W2T}$ is created (updated) each time we change the symbol table of the $\\lambda_{*LM}$.\n",
    "\n",
    "- If we do not plan to estimate $p(w_i|t_i)$ in $\\lambda_{W2T}$, we can create the FST as we did for $\\lambda_{W2T_{U}}$ (and keeping input symbol table the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create training data in utterance-per-line format for output symbols (t - tags)\n",
    "cat trn.conll | cut -f 2 |\\\n",
    "    sed 's/^$/~/g' | tr '\\n' ' ' | tr '~' '\\n' |\\\n",
    "    sed 's/  */ /g;s/^ *//g;s/ *$//g' > trn.t.txt\n",
    "\n",
    "# apply cut-off & recompile with OOV replace with `<unk>`\n",
    "ngramsymbols trn.t.txt trn.t.osyms.tmp\n",
    "farcompilestrings --symbols=trn.t.osyms.tmp --keep_symbols trn.t.txt trn.t.far.tmp\n",
    "ngramcount --order=1 trn.t.far.tmp trn.t.cnt.tmp\n",
    "ngramprint --integers trn.t.cnt.tmp trn.t.cnt.txt.tmp\n",
    "\n",
    "# filter lexicon\n",
    "while read -r word freq\n",
    "do\n",
    "    if (( freq > 1 ))\n",
    "    then \n",
    "        echo $word\n",
    "    fi\n",
    "done < trn.t.cnt.txt.tmp > trn.t.cnt.txt.cutoff.tmp\n",
    "\n",
    "# final output symbol table\n",
    "ngramsymbols trn.t.cnt.txt.cutoff.tmp t.osyms.txt\n",
    "\n",
    "# delete temp files\n",
    "rm -f *.all\n",
    "\n",
    "# compile data into FAR again\n",
    "farcompilestrings \\\n",
    "    --symbols=t.osyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.t.txt trn.t.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's train a unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states                                       1\n",
      "# of ngram arcs                                   38\n",
      "# of backoff arcs                                 0\n",
      "initial state                                     0\n",
      "unigram state                                     -1\n",
      "# of final states                                 1\n",
      "ngram order                                       1\n",
      "# of 1-grams                                      39\n",
      "well-formed                                       y\n",
      "normalized                                        y\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ngramcount --order=1 trn.t.far trn.t1.cnt\n",
    "ngrammake trn.t1.cnt t1.lm\n",
    "ngraminfo t1.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a new $\\lambda_{W2T}$ (let's call it $\\lambda_{W2T_{T}}$ for \"tags\"):\n",
    "    - following the same procedure we followed for $\\lambda_{W2T_{U}}$, but using:\n",
    "        - as input symbol table (`isyms.txt`)\n",
    "        - as output symbol table (`t.osyms.txt`)\n",
    "    - allowing `<unk> <unk>` and *word*-`<unk>` arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t('isyms.txt', 't.osyms.txt', out='w2t_t.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               t.osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         36138\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=t.osyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2t_t.txt w2t_t.bin\n",
    "\n",
    "fstinfo w2t_t.bin | head -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tO\t0.476737976\n",
      "1\t2\tof\tO\t0.476737976\n",
      "2\t3\t<unk>\tO\t0.476737976\n",
      "3\t2.00485039\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_t.bin | fstcompose - t1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 7117 tokens with 1091 phrases; found: 0 phrases; correct: 0.\n",
      "accuracy:  72.15%; precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
      "       actor.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "actor.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       actor.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   award.category: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   award.ceremony: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   character.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "     country.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    director.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "director.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      movie.genre: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.gross_revenue: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   movie.language: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   movie.location: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       movie.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.release_date: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.release_region: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.star_rating: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    movie.subject: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       movie.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      person.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    producer.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      rating.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t_t.bin | fstcompose - t1.lm |\\\n",
    "        fstshortestpath | fstrmepsilon | fsttopsort | fstprint\n",
    "done > w2t_t.t1.out\n",
    "\n",
    "paste tst.conll w2t_t.t1.out | cut  -f 1,2,6 | tr '\\t' ' ' | sed 's/^ .*//g' > w2t_t.t1.out.conll\n",
    "perl ../src/conlleval.pl < w2t_t.t1.out.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model still has $F_1=0$, since `O` is the tag with highest prior.\n",
    "- Observe the weights in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Exercises\n",
    "- Compare sizes of $\\lambda_{W2T_{U}}$ and $\\lambda_{W2T_{T}}$ for `# of arcs`\n",
    "- Unigram models & $\\lambda_{W2T}$\n",
    "    - Test pipeline: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{1}}$\n",
    "\n",
    "\n",
    "- Bigram models: train a *tag* bigram model (let's call it $\\lambda_{LM_{2}}$)\n",
    "\n",
    "    - Test pipeline with $\\lambda_{W2T_{U}}$: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{2}}$\n",
    "    - Test pipeline with $\\lambda_{W2T_{T}}$: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{T}} \\circ \\lambda_{LM_{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Maximum Likelihood Estimation (Emission Probabilities)\n",
    "- So far we haven't explored the relation between input and output\n",
    "- The next thing we can do is to expose our model to observations and estimate $p(w_{i}|t_{i})$ from data.\n",
    "- We can use `ngramcount` and `ngrammake` to make a smoothed probability model (we are using default, i.e. no parameters). \n",
    "- We need to estimate probabilities like we would estimate bigram probabilities, thus:\n",
    "    - prepare lexicon with *tags* and *words*\n",
    "    - read CoNLL format corpus into far (token per line, preprocessed)\n",
    "    - count bigrams\n",
    "    - make a bigram language model\n",
    "    - print bigrams with weights (negative log probabilities)\n",
    "    - choose bigrams (it will contain unigrams, as well as `<s>` and `</s>` bigrams)\n",
    "    - convert to FST & compile\n",
    "    \n",
    "- Let's call the model $\\lambda_{W2T_{MLE}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# lets use our symbol tables (since they both have been applied cut-off)\n",
    "cat isyms.txt t.osyms.txt | cut -f 1 | sort | uniq > msyms.text.tmp\n",
    "ngramsymbols msyms.text.tmp t.msyms.txt\n",
    "\n",
    "# let's convert data to ngrams\n",
    "cat trn.conll | sed '/^$/d' | awk '{print $2,$1}' > trn.w2t.txt\n",
    "\n",
    "# compile to far\n",
    "farcompilestrings \\\n",
    "    --symbols=t.msyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.w2t.txt trn.w2t.far\n",
    "    \n",
    "# count bigrams\n",
    "ngramcount --order=2 trn.w2t.far trn.w2t.cnt\n",
    "# make a model\n",
    "ngrammake trn.w2t.cnt trn.w2t.lm\n",
    "\n",
    "# print ngram probabilities as negative logs\n",
    "ngramprint \\\n",
    "    --symbols=t.msyms.txt\\\n",
    "    --negativelogs \\\n",
    "    trn.w2t.lm trn.w2t.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's define a python function to convert probabilities printout to W2T FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_w2t_mle(probs, out='w2t_mle.tmp'):\n",
    "    mcn = 3   # minimum column number\n",
    "    state = '0'\n",
    "    fs = \" \"  # column sepataror for fst\n",
    "    otag = 'O'\n",
    "    \n",
    "    lines = [line.strip().split(\"\\t\") for line in open(probs, 'r')]\n",
    "    \n",
    "    with open(out, 'w') as f:\n",
    "        for line in lines:\n",
    "            if len(line) == mcn:\n",
    "                if line[0].startswith(\"B-\") or line[0].startswith(\"I-\") or line[0] == otag:\n",
    "                    f.write(fs.join([state, state] + line) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_w2t_mle('trn.w2t.probs', out='w2t_mle.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               t.osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         1509\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompile \\\n",
    "    --isymbols=t.osyms.txt \\\n",
    "    --osymbols=isyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    trn.w2t.mle.txt w2t.mle.bin\n",
    "    \n",
    "# we need to invert it to have words on input\n",
    "fstinvert w2t.mle.bin w2t.mle.inv.bin\n",
    "\n",
    "fstinfo w2t.mle.inv.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-movie.name\t3.22130394\n",
      "1\t2\tof\tI-movie.name\t3.02857399\n",
      "2\t3\t<unk>\tB-director.nationality\t0.694147706\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t.mle.inv.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pipeline above represents \n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)}$$\n",
    "\n",
    "- To extend it to unigram tagging model we need to compose it with  $\\lambda_{LM_{1}} = p(t_i)$ \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-movie.name\t6.09392548\n",
      "1\t2\tof\tO\t3.86930203\n",
      "2\t3\t<unk>\tO\t4.46328497\n",
      "3\t2.00485039\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t.mle.inv.bin | fstcompose - t1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Maximum Likelihood Estimation\n",
    "- using `ngramprint` verify the Maximum Likelihood Estimation method (without `--negativelogs` it prints raw probabilities)\n",
    "    - print bigram counts from $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - print unigram counts for either from $\\lambda_{LM_{1}}$ or $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - using these counts compute probability of $p($ `brad|B-actor.name` $)$\n",
    "    - extract probability of $p($ `brad|B-actor.name` $)$ from $\\lambda_{W2T_{MLE}}$ (output of `ngrammake`)\n",
    "    - compare values\n",
    "    - repeat the procedure using counts from methods developed for the lab on ngram modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Markov Model Tagger\n",
    "- Evaluate the MLE pipeline using bigram model on tags, i.e.\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{2}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-1})}$$ \n",
    "\n",
    "- compare performances to the HMM tagger from previous lab (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Joint Distribution Modeling\n",
    "\n",
    "As we have seen, sequence labeling for Language Understanding could be approached using Hidden Markov Models (similar to Part-of-Speech Tagging), and to models it as in the table below (__HMM__). Stochastic Conceptual Language Models for Spoken Language Understanding in [Raymond & Riccardi (2007)](https://disi.unitn.it/~riccardi/papers2/IS07-GenerDiscrSLU.pdf) (__R&R__) model it jointly.\n",
    "\n",
    "\n",
    "| Model   | Equation |\n",
    "|:--------|:----------\n",
    "| __HMM__ | $$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})}$$\n",
    "| __R&R__ | $$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i}t_{i}|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "\n",
    "From implementation perspective, joint modeling implies the following:\n",
    "- we need to train $\\lambda_{SCLM}$ on word-tag pairs\n",
    "    - create corpus in a format for estimating $p(w_i,t_i|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})$\n",
    "    - create symbol tables\n",
    "- we need to change $\\lambda_{W2T}$ to output *word-tag* pairs (let's call it $\\lambda_{W2WT}$)\n",
    "    - create FST like above for $\\lambda_{W2WT}$ ($\\lambda_{W2WT_{WT}}$ - to differentiate from $\\lambda_{W2WT_{U}}$ that contains all possible combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Symbol Tables\n",
    "- Let's create symbol tables the same way we did for $\\lambda_{W2T_{T}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create training data in utterance-per-line format for output symbols (wt - tags)\n",
    "# using `+` to joint w & t\n",
    "cat trn.conll | tr '\\t' '+' |\\\n",
    "    sed 's/^$/~/g' | tr '\\n' ' ' | tr '~' '\\n' |\\\n",
    "    sed 's/  */ /g;s/^ *//g;s/ *$//g' > trn.wt.txt\n",
    "\n",
    "# apply cut-off & recompile with OOV replace with `<unk>`\n",
    "ngramsymbols trn.wt.txt trn.wt.osyms.tmp\n",
    "farcompilestrings --symbols=trn.wt.osyms.tmp --keep_symbols trn.wt.txt trn.wt.far.tmp\n",
    "ngramcount --order=1 trn.wt.far.tmp trn.wt.cnt.tmp\n",
    "ngramprint --integers trn.wt.cnt.tmp trn.wt.cnt.txt.tmp\n",
    "\n",
    "# filter lexicon\n",
    "while read -r word freq\n",
    "do\n",
    "    if (( freq > 1 ))\n",
    "    then \n",
    "        echo $word\n",
    "    fi\n",
    "done < trn.wt.cnt.txt.tmp > trn.wt.cnt.txt.cutoff.tmp\n",
    "\n",
    "# final output symbol table\n",
    "ngramsymbols trn.wt.cnt.txt.cutoff.tmp wt.osyms.txt\n",
    "\n",
    "# delete temp files\n",
    "rm -f *.all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's:\n",
    "    - compile our processed data into FAR\n",
    "    - train ngram language models on it - $\\lambda_{SCLM}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Conceptual Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states                                       1096\n",
      "# of ngram arcs                                   6179\n",
      "# of backoff arcs                                 1095\n",
      "initial state                                     1\n",
      "unigram state                                     0\n",
      "# of final states                                 534\n",
      "ngram order                                       2\n",
      "# of 1-grams                                      1095\n",
      "# of 2-grams                                      5618\n",
      "well-formed                                       y\n",
      "normalized                                        y\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# compile data into FAR again\n",
    "farcompilestrings \\\n",
    "    --symbols=wt.osyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.wt.txt trn.wt.far\n",
    "\n",
    "# train ngram model\n",
    "ngramcount --order=2 trn.wt.far trn.wt.cnt\n",
    "ngrammake trn.wt.cnt wt2.lm\n",
    "ngraminfo wt2.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building W2WT FST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's build unweighted $\\lambda_{W2WT_{WT}}$, using\n",
    "    - input symbol table `isyms.txt`\n",
    "    - output symbol table `wt.osyms.txt`\n",
    "- all words that appear in `isyms.txt` and do not appear in *word-tag* pairs (i.e. `wt.syms.txt`) need to be taken care of\n",
    "    - in practice this means that we need do either of the two:\n",
    "        - update input symbol table, removing these words\n",
    "        - construct $\\lambda_{W2T}$ that handles that (mapping them to '`<unk>`', as there are no '`<unk>+iob+type`')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2t_filter(isyms, osyms, out='w2t.tmp', sep='+'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'\n",
    "    state = '0'\n",
    "    fs = \" \"  # column sepataror for fst\n",
    "    \n",
    "    ist = set([line.strip().split(\"\\t\")[0] for line in open(isyms, 'r')]) - special\n",
    "    ost = set([line.strip().split(\"\\t\")[0] for line in open(osyms, 'r')]) - special\n",
    "    known = set()\n",
    "    \n",
    "    with open(out, 'w') as f: \n",
    "        for e in list(ost):\n",
    "            if e == oov:\n",
    "                f.write(fs.join([state, state, e, e]) + \"\\n\")  # <unk> <unk>\n",
    "            else:\n",
    "                w,t = e.split(sep)\n",
    "                known.add(w)\n",
    "                f.write(fs.join([state, state, w, e]) + \"\\n\")\n",
    "        \n",
    "        for e in list(ist):\n",
    "            if e not in known:\n",
    "                f.write(fs.join([state, state, e, oov]) + \"\\n\")\n",
    "    \n",
    "        f.write(state + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_w2t_filter('isyms.txt', 'wt.osyms.txt', out='w2wt_wt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               wt.osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         1157\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=wt.osyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2wt_wt.txt w2wt_wt.bin\n",
    "\n",
    "fstinfo w2wt_wt.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets test the whole $\\lambda_{W} \\circ \\lambda_{W2WT_{WT}} \\circ \\lambda_{SCLM_{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tstar+O\t7.93915796\n",
      "1\t2\tof\tof+O\t1.55418813\n",
      "2\t3\t<unk>\t<unk>\t2.84977818\n",
      "3\t1.10391009\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2wt_wt.bin | fstcompose - wt2.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "- Since on the output we have `word+tag`, we need to post-process the output for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Full $\\lambda_{W2WT_{U}}$\n",
    "- Implement $\\lambda_{W2WT_{U}}$ using 'full' input and output symbol tables (`isyms.txt` and `osyms.txt`)\n",
    "- Test the pipeline: $\\lambda_{W} \\circ \\lambda_{W2WT_{U}} \\circ \\lambda_{SCLM_{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
