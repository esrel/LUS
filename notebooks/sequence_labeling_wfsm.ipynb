{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling with Weighted Finite State Transducers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language Understanding Systems Lab\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Spoken/Natural) Language Understanding with WFST\n",
    "\n",
    "$$\\lambda_{LU} = \\lambda_{W} \\circ \\lambda_{G} \\circ \\lambda_{W2T} \\circ \\lambda_{SCLM}$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda_{W}$\n",
    "- $\\lambda_{G}$\n",
    "- $\\lambda_{W2T}$\n",
    "- $\\lambda_{SCML}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Distribution Modeling (Dinarelli et. al)\n",
    "\n",
    "As we have seen, sequence labeling for Natural Language Understanding could be approached using Hidden Markov Models (similar to Part-of-Speech Tagging, will modifications, i.e. O --> w)\n",
    "\n",
    "\n",
    "\n",
    "| Model   | Equation |\n",
    "|:--------|:----------\n",
    "| __HMM__ | $$p(t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})}$$\n",
    "| __R&R__ | $$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i}t_{i}|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dinarelli proposes to model $p(t_{1}^{n})$ jointly as $p(w_{1})$\n",
    "\n",
    "- join words & tags together\n",
    "- train lm\n",
    "- use them together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TM + LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\ta\n",
      "0\t1\tb\n",
      "0\t1\tc\n",
      "0\t1\td\n",
      "0\t1\te\n",
      "0\t1\tf\n",
      "0\t1\tg\n",
      "0\t1\th\n",
      "0\t1\ti\n",
      "0\t1\tj\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "function char_lex() {\n",
    "    # epsilon, unknown, space\n",
    "    echo -e \"<eps>\\t0\"\n",
    "    echo -e \"<unk>\\t1\"\n",
    "    echo -e \"<space>\\t2\"\n",
    "\n",
    "    cnt=3\n",
    "\n",
    "    # lowercase letters\n",
    "    for c in {a..z}\n",
    "    do\n",
    "        echo -e \"$c\\t$cnt\"\n",
    "        ((cnt++))\n",
    "    done\n",
    "\n",
    "    # capital letters\n",
    "    for c in {A..Z}\n",
    "    do\n",
    "        echo -e \"$c\\t$cnt\"\n",
    "        ((cnt++))\n",
    "    done\n",
    "}\n",
    "\n",
    "function fsa_e1a() {\n",
    "    for c in {a..z}\n",
    "    do\n",
    "        echo \"0 1 $c\"\n",
    "    done\n",
    "\n",
    "    # uppercase letters\n",
    "    for c in {A..Z}\n",
    "    do\n",
    "        echo \"0 1 $c\"\n",
    "    done\n",
    "\n",
    "    # space\n",
    "    echo \"0 1 <space>\"\n",
    "\n",
    "    # final state is 1\n",
    "    echo \"1\"\n",
    "}\n",
    "\n",
    "char_lex > e1.sym\n",
    "fsa_e1a > e1a.txt\n",
    "\n",
    "fstcompile --acceptor --isymbols=e1.sym --keep_isymbols e1a.txt e1a.bin\n",
    "fstprint --acceptor e1a.bin | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
