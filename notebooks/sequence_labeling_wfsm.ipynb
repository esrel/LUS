{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling with Weighted Finite State Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Language Understanding Systems Lab\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the Laboratory Work for [Language Understanding Systems class](http://disi.unitn.it/~riccardi/page7/page13/page13.html) of [University of Trento](https://www.unitn.it/en).\n",
    "Laboratory has been ported to jupyer notebook format for remote teaching during [COVID-19 pandemic](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [OpenFST](http://www.openfst.org/twiki/bin/view/FST/WebHome)\n",
    "- [OpenGRM](http://www.opengrm.org/twiki/bin/view/GRM/NGramLibrary)\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Shallow Parsing with WFSMs: Natural Language Understanding\n",
    "\n",
    "Language Understanding of several tasks, one of which is entity extraction (concept tagging). The task is usually approached as Shallow Parsing, where we segment the input into constituents and label them using IOB-schemes.\n",
    "\n",
    "Using Weighted Finite State Machines for the task provides several benefits:\n",
    "- Even though we saw how to do sequence labeling using HMM, in real applications models can become quite complex to solve. \n",
    "- The task usually involves several components (e.g. *emission* & *transition* probabilities), and WFSMs provide an efficient way to represent and process this components via intersection and composition operations.\n",
    "    - WFSTs are good at modeling HMM and solving state machine problems\n",
    "    - Weights can be associated with edges as costs or probabilities (default: cost = negative log probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Common Sequence Labeling Pipeline\n",
    "\n",
    "The common approach to concept tagging (or sequence labeling in general) makes use of 3 components:\n",
    "\n",
    "|                   | Description                      \n",
    "|:------------------|:------------------------------\n",
    "| $$\\lambda_{W}$$   | FSA representation of an input sentence\n",
    "| $$\\lambda_{W2T}$$ | FST to translate words into output labels (e.g. `iob+type`)\n",
    "| $$\\lambda_{*LM}$$ | FSA Ngram Language Model to score the sequences of output labels\n",
    "\n",
    "Consequently, Sequence Labeling ($\\lambda$) is performed by composition of these three components as:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T} \\circ \\lambda_{*LM}$$\n",
    "\n",
    "- It is common to include other components to perform intermediate operations for:\n",
    "    - generalization of input ($\\lambda_{G}$)\n",
    "    - cleaning of output\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. General Setup\n",
    "Let's start by preparing our workspace.\n",
    "\n",
    "What we have is:\n",
    "- training & test sets in utterance-per-line format\n",
    "- training & test sets in CoNLL format that contain word-tag observations\n",
    "\n",
    "For working with WFSMs we need:\n",
    "- input symbol table: words\n",
    "- output symbol table: tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Preprocessing:\n",
    "To handle OOV (unknown) words let's:\n",
    "- apply frequency cut-off to lexicon\n",
    "- replace OOV words in both training and test data with `<unk>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOV with Frequency Cut-off using OpenGRM NGram Library Tools\n",
    "\n",
    "It is easy to apply frequency cut-off and replace OOV with `<unk>` using other means (e.g. `python`). \n",
    "Here we demonstrate how to achieve that using provided tools and some `unix` commands.\n",
    "\n",
    "__Objective__: map low frequency unigrams to `<unk>`\n",
    "\n",
    "- generate symbol table using `ngramsymbols` for a corpus\n",
    "- compile the corpus into FAR using `farcompilestrings`\n",
    "- count unigrams in FAR using `ngramcount`\n",
    "- print counts using `ngramprint`\n",
    "- filter the words externally and save them into a file\n",
    "- generate a new symbol table using `ngramsymbols`\n",
    "- recompile the corpus into a new FAR using the new symbol table and `farcompilestrings`\n",
    "\n",
    "> __Note__: *if you provide an external word list to `ngramsymbols` it will generate a symbol table in the required format.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we will be using corpus files a lot, let's copy them into current directory with shorter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dpath='NL2SparQL4NLU/dataset/NL2SparQL4NLU'\n",
    "\n",
    "cp $dpath.train.utterances.txt trn.txt\n",
    "cp $dpath.test.utterances.txt tst.txt\n",
    "\n",
    "cp $dpath.train.conll.txt trn.conll\n",
    "cp $dpath.test.conll.txt tst.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling OOV with OpenFST and OpenGRM tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# creat full symbol table\n",
    "ngramsymbols trn.txt trn.isyms.tmp\n",
    "# compile into FAR\n",
    "farcompilestrings --symbols=trn.isyms.tmp --keep_symbols trn.txt trn.far.tmp\n",
    "# count unigrams\n",
    "ngramcount --order=1 trn.far.tmp trn.cnt.tmp\n",
    "# print counts as integers\n",
    "ngramprint --integers trn.cnt.tmp trn.cnt.txt.tmp\n",
    "\n",
    "# bash: you can use python to process file\n",
    "while read -r word freq; do \\\n",
    "    if (( freq > 1 )); then echo $word ; fi \\\n",
    "done < trn.cnt.txt.tmp > trn.cnt.txt.cutoff.tmp\n",
    "\n",
    "# final input symbol table\n",
    "ngramsymbols trn.cnt.txt.cutoff.tmp isyms.txt\n",
    "\n",
    "# delete temp files\n",
    "rm *.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# let's compile both training and test set into far using this symbol table\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.txt trn.far\n",
    "\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    tst.txt tst.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have:\n",
    "- Symbol table (`isyms.txt`)\n",
    "    - contains `['<s>', '</s>', '<epsilon>', '<unk>']` that are added automatically\n",
    "- Training data as FAR with OOV replaced (`trn.far`)\n",
    "- Test data as FAR with OOV replaced (`tst.far`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating Lexicon from CoNLL Format Corpus\n",
    "To do sequence labeling we additionally require *output symbol table*.\n",
    "We can generate it using simple `unix` commands and `ngramsymbols` from CoNLL format corpora.\n",
    "- create a unique list of output symbols using `unix`\n",
    "- generate symbol table using `ngramsymbols`\n",
    "\n",
    "Even though our test set might contain __types__ not present in the training set, since they are required only for evaluation, our symbol table can ignore them. \n",
    "\n",
    "Since our output labels are composed of segmentation(`iob`) and classification (`type`) labels, we can make sure that each __type__ has all possible IOB prefixes (even those not present in training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# create a unique tag list\n",
    "cat trn.conll | cut -f 2 | cut -d '-' -f 2 | sed '/^ *$/d' | sort | uniq > types.txt\n",
    "\n",
    "while read -r word\n",
    "do\n",
    "    if [[ $word != 'O' ]]\n",
    "    then\n",
    "        echo \"B-$word\"\n",
    "        echo \"I-$word\"\n",
    "    else\n",
    "        echo $word\n",
    "    fi\n",
    "done < types.txt > osyms.tmp\n",
    "\n",
    "# generate symbol table with tags\n",
    "ngramsymbols osyms.tmp osyms.txt\n",
    "\n",
    "rm *.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Applying FSTs\n",
    "- Our objective is to be able to annotate input sentences (as FSAs in FAR) using the machines we are going to build. \n",
    "- Our test data has been loaded into FAR archive\n",
    "- We will can `farextract` to extract these sentence FSAs and apply our FSMs to them\n",
    "    - `farextract --filename_prefix=\"<odir>\" <FAR>` will extract contents of `<FAR>` into directory `<odir>`\n",
    "    - we can iterate over files in the directory and apply operations (see below)\n",
    "- We can also create FAR of the processed FSMs using `farcreate` as\n",
    "    - `farcreate --file_list_input <list of FST filenames> <output FAR>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tstar\n",
      "1\t2\tof\tof\n",
      "2\t3\t<unk>\t<unk>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "mkdir -p $wdir\n",
    "\n",
    "farextract --filename_prefix=\"$wdir/\" tst.far\n",
    "cp $wdir/tst.txt-0001 sent.fsa\n",
    "\n",
    "fstprint sent.fsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For testing we are going to use this `sent.fsa`\n",
    "- For evaluation we are going to iterate over whole FAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Baselines\n",
    "Let's demonstrate the process by building some simple Shallow Parsing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Random\n",
    "The simplest solution is to assign output labels randomly. \n",
    "\n",
    "To achieve this we need to implement an FST that translates our words into output symbols ($\\lambda_{W2T}$) with equal cost, or no cost at all (i.e. unweighted FST -- let's call it $\\lambda_{U}$ for \"unweighted\"). \n",
    "\n",
    "The resulting FST represents search space for $p(w_i|t_i)$ without being exposed to any observation. It is build using only our knowledge of input and output symbols and all translations are possible. \n",
    "\n",
    "Since we have no model yet, the whole pipeline is:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         44697\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's read our symbol tables into arrays, removing special symbols except '<unk>' for input\n",
    "# and removing all of them for output (to avoid <unk> - <unk>, which we can still keep)\n",
    "ist_arr=($(cat isyms.txt | sed '/<epsilon>/d;/<s>/d;/<\\/s>/d' | cut -f 1 ))\n",
    "ost_arr=($(cat osyms.txt | sed '/<epsilon>/d;/<s>/d;/<\\/s>/d;/<unk>/d' | cut -f 1 ))\n",
    "\n",
    "# creating transducer\n",
    "for is in ${ist_arr[@]}\n",
    "do\n",
    "    for os in ${ost_arr[@]}\n",
    "    do\n",
    "        echo \"0 0 $is $os\"\n",
    "    done\n",
    "done > w2t.u.txt\n",
    "# add final state\n",
    "echo \"0\" >> w2t.u.txt\n",
    "\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=osyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2t.u.txt w2t.u.bin\n",
    "\n",
    "fstinfo w2t.u.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- Compute input and output symbol table sizes\n",
    "- Compare their multiplication to `# of arcs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tO\n",
      "1\t2\tof\tO\n",
      "2\t3\t<unk>\tO\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t.u.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- For evaluation we are going to use `conlleval.pl` script (provided is `src` directory)\n",
    "- We need to convert output to the appropriate format\n",
    "- All tokens will be predicted as `O`.\n",
    "    - $F_1=0$\n",
    "    - Bonus Question: *Why?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t.u.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint\n",
    "done > w2t.u.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 7117 tokens with 1091 phrases; found: 0 phrases; correct: 0.\n",
      "accuracy:  72.15%; precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
      "       actor.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "actor.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       actor.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   award.category: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   award.ceremony: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   character.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "     country.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    director.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "director.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      movie.genre: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.gross_revenue: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   movie.language: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   movie.location: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       movie.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.release_date: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.release_region: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.star_rating: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    movie.subject: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       movie.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      person.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    producer.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      rating.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "paste tst.conll w2t.u.out | cut  -f 1,2,6 | tr '\\t' ' ' | sed 's/^ .*//g' > w2t.u.out.conll \n",
    "perl ../src/conlleval.pl < w2t.u.out.conll                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Output Symbol Priors\n",
    "\n",
    "The simplest form for the third component is to use output label priors, i.e. unigram probabilities of output labels. To model that we can:\n",
    "- train a unigram language model using `ngramcount` & `ngrammake` (let's call it $\\lambda_{LM_{1}}$)\n",
    "- compose it with the *random* $\\lambda_{W2T_{U}}$ so that the whole pipeline becomes:\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "- In this baseline our model becomes exposed to observations\n",
    "- Since `O` tag is the most frequent & we will have a model that always predicts it.\n",
    "- We can represent our model as:\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i)}$$\n",
    "\n",
    "Let's create corpus of output labels in sentence-per-line format & create FAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat trn.conll | cut -f 2 |\\\n",
    "    sed 's/^$/~/g' | tr '\\n' ' ' | tr '~' '\\n' |\\\n",
    "    sed 's/  */ /g;s/^ *//g;s/ *$//g' > trn.t.txt\n",
    "\n",
    "farcompilestrings \\\n",
    "    --symbols=osyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.t.txt trn.t.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's train a unigram language model and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states                                       1\n",
      "# of ngram arcs                                   41\n",
      "# of backoff arcs                                 0\n",
      "initial state                                     0\n",
      "unigram state                                     -1\n",
      "# of final states                                 1\n",
      "ngram order                                       1\n",
      "# of 1-grams                                      42\n",
      "well-formed                                       y\n",
      "normalized                                        y\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ngramcount --order=1 trn.t.far trn.t.1.cnt\n",
    "ngrammake trn.t.1.cnt t.1.lm\n",
    "ngraminfo t.1.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tO\t0.476737976\n",
      "1\t2\tof\tO\t0.476737976\n",
      "2\t3\t<unk>\tO\t0.476737976\n",
      "3\t2.00485039\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t.u.bin | fstcompose - t.1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 7117 tokens with 1091 phrases; found: 0 phrases; correct: 0.\n",
      "accuracy:  72.15%; precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
      "       actor.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "actor.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       actor.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   award.category: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   award.ceremony: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   character.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "     country.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    director.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "director.nationality: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      movie.genre: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.gross_revenue: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   movie.language: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "   movie.location: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       movie.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.release_date: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.release_region: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "movie.star_rating: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    movie.subject: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "       movie.type: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      person.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "    producer.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "      rating.name: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t.u.bin | fstcompose - t.1.lm |\\\n",
    "        fstshortestpath | fstrmepsilon | fsttopsort | fstprint\n",
    "done > w2t.u.t.1.out\n",
    "\n",
    "paste tst.conll w2t.u.t.1.out | cut  -f 1,2,6 | tr '\\t' ' ' | sed 's/^ .*//g' > w2t.u.t.1.out.conll\n",
    "perl ../src/conlleval.pl < w2t.u.t.1.out.conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model still has $F_1=0$, since `O` is the tag with highest prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Bigram Model: Exercise\n",
    "- Train a *tag* bigram model (let's call it $\\lambda_{LM_{2}}$)\n",
    "- Test pipeline with $\\lambda_{W2T_{U}}$ as \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{2}}$$\n",
    "\n",
    "- Evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Maximum Likelihood Estimation (Emission Probabilities)\n",
    "- So far we haven't explored the relation between input and output\n",
    "- The next thing we can do is to expose our model to observations and estimate $p(w_{i}|t_{i})$ from data.\n",
    "- We can use `ngramcount` and `ngrammake` to make a smoothed probability model (we are using default, i.e. no parameters). \n",
    "- We need to estimate probabilities like we would estimate bigram probabilities, thus:\n",
    "    - prepare lexicon with *tags* and *words*\n",
    "    - read CoNLL format corpus into far (token per line, preprocessed)\n",
    "    - count bigrams\n",
    "    - make a bigram language model\n",
    "    - print bigrams with weights (negative log probabilities)\n",
    "    - choose bigrams (it will contain unigrams, as well as `<s>` and `</s>` bigrams)\n",
    "    - convert to FST & compile\n",
    "    \n",
    "- Let's call them model $\\lambda_{W2T_{MLE}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         1513\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# lets use our symbol tables\n",
    "cat isyms.txt osyms.txt | cut -f 1 | sort | uniq > msyms.text.tmp\n",
    "ngramsymbols msyms.text.tmp msyms.txt\n",
    "\n",
    "# let's convert data to ngrams\n",
    "cat trn.conll | sed '/^$/d' | awk '{print $2,$1}' > trn.w2t.txt\n",
    "\n",
    "# compile to far\n",
    "farcompilestrings \\\n",
    "    --symbols=msyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.w2t.txt trn.w2t.far\n",
    "    \n",
    "# count bigrams\n",
    "ngramcount --order=2 trn.w2t.far trn.w2t.cnt\n",
    "# make a model\n",
    "ngrammake trn.w2t.cnt trn.w2t.lm\n",
    "\n",
    "# print ngram probabilities as negative logs\n",
    "ngramprint \\\n",
    "    --symbols=msyms.txt\\\n",
    "    --negativelogs \\\n",
    "    trn.w2t.lm trn.w2t.probs\n",
    "\n",
    "# since the model also contains unigrams and initial and final symbols ngrams, \n",
    "# let's get ngrams starting with iob-prefix\n",
    "cat trn.w2t.probs | grep '^[IB]-\\|O ' | tr '\\t' ' ' | sed 's/^/0 0 /g' > tmp.tmp\n",
    "\n",
    "# remove remaining unigrams\n",
    "while read line\n",
    "do\n",
    "    len=$(echo \"$line\" | grep -o \" \" | wc -l)\n",
    "    # 4 space == 5 columns\n",
    "    if (( len == 4 ))\n",
    "    then\n",
    "        echo \"$line\"\n",
    "    fi\n",
    "done < tmp.tmp > trn.w2t.mle.txt\n",
    "\n",
    "# adding final state\n",
    "echo '0' >> trn.w2t.mle.txt\n",
    "\n",
    "fstcompile \\\n",
    "    --isymbols=osyms.txt \\\n",
    "    --osymbols=isyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    trn.w2t.mle.txt w2t.mle.bin\n",
    "    \n",
    "# we need to invert it to have words on input\n",
    "fstinvert w2t.mle.bin w2t.mle.inv.bin\n",
    "\n",
    "fstinfo w2t.mle.inv.bin | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-movie.name\t3.22130394\n",
      "1\t2\tof\tI-movie.name\t3.02857399\n",
      "2\t3\t<unk>\tB-director.nationality\t0.694147706\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t.mle.inv.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The pipeline above represents \n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)}$$\n",
    "\n",
    "- To extend it to unigram tagging model we need to compose it with  $\\lambda_{LM_{1}} = p(t_i)$ \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\tstar\tB-movie.name\t6.09392548\n",
      "1\t2\tof\tO\t3.86930203\n",
      "2\t3\t<unk>\tO\t4.46328497\n",
      "3\t2.00485039\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t.mle.inv.bin | fstcompose - t.1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Maximum Likelihood Estimation\n",
    "- using `ngramprint` verify the Maximum Likelihood Estimation method\n",
    "    - print bigram counts for $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - print unigram counts for $\\lambda_{LM_{1}}$ (output of `ngramcount`)\n",
    "    - compute negative log probabilities using those counts\n",
    "    - compare values to the ones extracted from the $\\lambda_{W2T_{MLE}}$ (output of `ngrammake`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: HMM Tagger\n",
    "- Evaluate the MLE pipeline using bigram model on tags, i.e.\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{2}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-1})}$$ \n",
    "\n",
    "- compare performances to the HMM tagger from previous lab (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Joint Distribution Modeling\n",
    "\n",
    "As we have seen, sequence labeling for Language Understanding could be approached using Hidden Markov Models (similar to Part-of-Speech Tagging), and to models it as in the table below (__HMM__). Stochastic Conceptual Language Models for Spoken Language Understanding in [Raymond & Riccardi (2007)](https://disi.unitn.it/~riccardi/papers2/IS07-GenerDiscrSLU.pdf) (__R&R__) model it jointly.\n",
    "\n",
    "\n",
    "| Model   | Equation |\n",
    "|:--------|:----------\n",
    "| __HMM__ | $$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})}$$\n",
    "| __R&R__ | $$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i}t_{i}|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "\n",
    "From implementation perspective, joint modeling implies the following:\n",
    "- we need to train $\\lambda_{SCLM}$ on word-tag pairs\n",
    "    - create corpus in a format for estimating $p(w_i,t_i|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})$\n",
    "    - create symbol tables\n",
    "- we need to change $\\lambda_{W2T}$ to output *word-tag* pairs (let's call it $\\lambda_{W2WT}$)\n",
    "    - create FST like above for $\\lambda_{W2WT}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Symbol Tables\n",
    "- Let's create symbol tables the same way we did for $\\lambda_{W2T_{U}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fst type                                          vector\n",
      "arc type                                          standard\n",
      "input symbol table                                isyms.txt\n",
      "output symbol table                               wt.osyms.txt\n",
      "# of states                                       1\n",
      "# of arcs                                         44697\n",
      "initial state                                     0\n",
      "# of final states                                 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's read our symbol tables into arrays, removing special symbols except '<unk>' for input\n",
    "# and removing all of them for output (to avoid <unk> - <unk>, which we can still keep)\n",
    "ist_arr=($(cat isyms.txt | sed '/<epsilon>/d;/<s>/d;/<\\/s>/d' | cut -f 1 ))\n",
    "ost_arr=($(cat osyms.txt | sed '/<epsilon>/d;/<s>/d;/<\\/s>/d;/<unk>/d' | cut -f 1 ))\n",
    "\n",
    "# creating transducer\n",
    "for is in ${ist_arr[@]}\n",
    "do\n",
    "    for os in ${ost_arr[@]}\n",
    "    do\n",
    "        echo \"0 0 $is ${is}+${os}\"\n",
    "    done\n",
    "done > w2wt.u.txt\n",
    "# add final state\n",
    "echo \"0\" >> w2wt.u.txt\n",
    "\n",
    "# creating input for output symbol tables\n",
    "for is in ${ist_arr[@]}\n",
    "do\n",
    "    for os in ${ost_arr[@]}\n",
    "    do\n",
    "        echo \"${is}+${os}\"\n",
    "    done\n",
    "done > wt.osyms.tmp\n",
    "\n",
    "ngramsymbols wt.osyms.tmp wt.osyms.txt\n",
    "rm *.tmp\n",
    "\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=wt.osyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2wt.u.txt w2wt.u.bin\n",
    "\n",
    "fstinfo w2wt.u.bin | head -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# using `~` for sentence separation since it is not in data\n",
    "# using `+` to represet W+T (like in symbol table)\n",
    "\n",
    "cat trn.conll | tr '\\t' '+' |\\\n",
    "    sed 's/^$/~/g' | tr '\\n' ' ' | tr '~' '\\n' |\\\n",
    "    sed 's/  */ /g;s/^ *//g;s/ *$//g' > trn.wt.txt\n",
    "\n",
    "farcompilestrings \\\n",
    "    --symbols=wt.osyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<unk>' \\\n",
    "    trn.wt.txt trn.wt.far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's train an ngram model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of states                                       1484\n",
      "# of ngram arcs                                   7073\n",
      "# of backoff arcs                                 1483\n",
      "initial state                                     1\n",
      "unigram state                                     0\n",
      "# of final states                                 638\n",
      "ngram order                                       2\n",
      "# of 1-grams                                      1483\n",
      "# of 2-grams                                      6228\n",
      "well-formed                                       y\n",
      "normalized                                        y\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ngramcount --order=2 trn.wt.far trn.wt.cnt\n",
    "ngrammake trn.wt.cnt wt.2.lm\n",
    "ngraminfo wt.2.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets test the whole $\\lambda_{W} \\circ \\lambda_{W2WT} \\circ \\lambda_{SCLM_{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2wt.u.bin | fstcompose - wt.2.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
