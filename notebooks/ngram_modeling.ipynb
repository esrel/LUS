{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Modeling\n",
    "\n",
    "- Language Understanding Systems\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the Laboratory Work for [Language Understanding Systems class](http://disi.unitn.it/~riccardi/page7/page13/page13.html) of [University of Trento](https://www.unitn.it/en).\n",
    "Laboratory has been ported to jupyer notebook format for remote teaching during [COVID-19 pandemic](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic).\n",
    "\n",
    "Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/)) is advised for reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Section *Corpora and Counting* covers some concepts of *Chapter 2: \"Regular Expressions, Text Normalization, Edit Distance\"*.\n",
    "- Sections *Ngrams and Ngram Probabilities* and *Language Models* roughly cover *Chapter 3: \"N-gram Language Models\"*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset\n",
    "\n",
    "    - run `git clone https://github.com/esrel/NL2SparQL4NLU.git`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL2SparQL4NLU\n",
    "\n",
    "- __Format__:\n",
    "\n",
    "    - Utterance (sentence) per line\n",
    "    - Tokenized\n",
    "    - Lowercased\n",
    "\n",
    "- __Language__: English monolingual\n",
    "\n",
    "- __Annotation__: None (for now)\n",
    "\n",
    "- __Split__: training & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to load a corpus into a list-of-lists\n",
    "\n",
    "- load `NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt`\n",
    "- print first `2` words (tokens) of the first `10` sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
    "tst='NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* description in terms of:\n",
    "\n",
    "- total number of words\n",
    "- total number of utterances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute corpus descriptive statistics -- total utterance and word counts.\n",
    "- compute the statistics for the __training__ and __test__ sets of NL2SparQL4NLU dataset. \n",
    "- compare the computed statistics with the reference values below.\n",
    "\n",
    "\n",
    "| Metric           | Train  | Test   |\n",
    "|------------------|-------:|-------:|\n",
    "| Total Words      | 21,453 |  7,117 |\n",
    "| Total Utterances |  3,338 |  1,084 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute a lexicon from corpus in a list-of-lists format\n",
    "    - sort the list alphabetically\n",
    "    \n",
    "- compute the lexicon of the training set of NL2SparQL4NLU dataset\n",
    "- compare the its size to the reference value below.\n",
    "\n",
    "| Metric       | Value |\n",
    "|--------------|------:|\n",
    "| Lexicon Size | 1,729 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts (our corpus is lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute a frequency list for a corpus\n",
    "- compute frequency list for the training set of NL2SparQL4NLU dataset\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compare the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| the    |     1,337 |\n",
    "| movies |     1,126 |\n",
    "| of     |       607 |\n",
    "| in     |       582 |\n",
    "| movie  |       564 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*).\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Cut-Off\n",
    "\n",
    "- define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "\n",
    "    - use default values for min and max\n",
    "    \n",
    "- using frequency list for the training set of NL2SparQL4NLU dataset\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 1729 |\n",
    "| cut-off    |   2 | N/A |  950 |\n",
    "| cut-off    | N/A | 100 | 1694 |\n",
    "| cut-off    |   2 | 100 |  915 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop Word Removal\n",
    "\n",
    "- define a function to read/load a list of words in token-per-line format (i.e. lexicon)\n",
    "- load stop word list from `NL2SparQL4NLU/extras/english.stop.txt`\n",
    "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    \n",
    "    - define a function to compute overlap of two lexicons\n",
    "    - define a function to apply a stopword list to a lexicon\n",
    "\n",
    "- compare the 100 most frequent words in frequency list of the training set to the list of stopwords (report count)\n",
    "- apply stopword list to the lexicon of the training set\n",
    "- report size of the resulting lexicon comparing to the reference values.\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 1729 |\n",
    "| no stop words   | 1529 |\n",
    "| top 100 overlap |   50 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swl='NL2SparQL4NLU/extras/english.stop.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ngrams and Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of *n* items from a given sequence of text or speech. An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character n-grams: cat\n",
    "- word n-grams: the cat is fat\n",
    "\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['c', 'a', 't']` | `['ca', 'at']` | `['cat']` |\n",
    "| *Word N-grams*      | `['the', 'cat', 'is' , 'fat']` | `['the cat', 'cat is', ...]` | `['the cat is', ...]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Counting Ngrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count. Ngram count only differs in a unit of counting -- sequence of $n$ of words. We can compute bigram count by taking sequences of 2 items, trigrams - 3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Data Structures\n",
    "For *Frequency List* we have used [`dict`](https://docs.python.org/2/library/stdtypes.html#dict) to store counts.\n",
    "In ngram counting scenario we still can use dictionary, and use ngram string keys; however, there are better data structures for the task. The frequent data structures used to store ngram counts or probabilities are [hash table](https://en.wikipedia.org/wiki/Hash_table), [trie](https://en.wikipedia.org/wiki/Trie), or [finite state automaton](https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton). The pros and cons of each data structure are out of the scope of this lab; for the discussion on efficient data structures for language modeling please refer to [KenLM paper](https://kheafield.com/papers/avenue/kenlm.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Implementing Trie\n",
    "There are plenty of implementation of the Trie data structure (e.g. [`pygtrie`](https://github.com/google/pygtrie/) from Google). However, it is simple enough. \n",
    "\n",
    "For understanding purposes let's implement our own, such that:\n",
    "\n",
    "- works on lists of words\n",
    "- stores in a node: \n",
    "    - word\n",
    "    - its count\n",
    "    - children (next words)\n",
    "- implements methods to:\n",
    "    - add a sequence (updating counts)\n",
    "    - get a node by a sequence (i.e. prefix)\n",
    "    - traverse a trie and get all sequences\n",
    "        - allows to traverse children of any node\n",
    "    - compute size of ngram vocabulary (V)\n",
    "    \n",
    "It is convenient to introduce an `oov` node to the Trie to easily handle sequences not in it.\n",
    "Thus, we also add `self.ovv` (with `count` set to $0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "    def __init__(self, word=None):\n",
    "        self.word = word\n",
    "        self.children = {}\n",
    "        self.count = 0\n",
    "\n",
    "    def __set__(self, instance, value):\n",
    "        self.instance = value\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        return self.instance\n",
    "\n",
    "\n",
    "class Trie(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = Node('*')  # trie root\n",
    "        self.oov = Node()      # node for oov values\n",
    "        self.size = 0          # depth of trie\n",
    "\n",
    "    def __set__(self, instance, value):\n",
    "        self.instance = value\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        return self.instance\n",
    "\n",
    "    def add(self, sequence):\n",
    "        node = self.root\n",
    "        node.count += 1  # total word count\n",
    "        for word in sequence:\n",
    "            node.children[word] = node.children.setdefault(word, Node(word))\n",
    "            node = node.children[word]\n",
    "            node.count += 1\n",
    "\n",
    "    def get(self, sequence):\n",
    "        node = self.root\n",
    "        for word in sequence:\n",
    "            node = node.children.get(word, self.oov)\n",
    "        return node\n",
    "\n",
    "    def traverse(self, node=None, sequence=None, size=None):\n",
    "        sequence = sequence if sequence else []\n",
    "        node = self.root if not node else node\n",
    "\n",
    "        if not node.children:\n",
    "            yield sequence\n",
    "\n",
    "        if size:\n",
    "            if len(sequence) == size:\n",
    "                yield sequence\n",
    "\n",
    "        for word, n in node.children.items():\n",
    "            sequence.append(word)\n",
    "            yield from self.traverse(n, sequence, size=size)\n",
    "            sequence.pop()\n",
    "\n",
    "    def v(self, size=None):\n",
    "        return len(list(self.traverse(size=size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing out the implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram size: 4\n",
      "3 ['a']\n",
      "2 ['a', 'b']\n",
      "0 ['a', 'x']\n",
      "0 ['e', 'c', 'f']\n",
      "1 ['a', 'e', 'c', 'f']\n",
      "All In Trie:\n",
      "['a', 'b', 'c', 'd']\n",
      "['a', 'b', 'g', 'h']\n",
      "['a', 'e', 'c', 'f']\n",
      "['x', 'y', 'z', 'a']\n",
      "In Trie for: ['a']\n",
      "1 ['a', 'b', 'c', 'd']\n",
      "1 ['a', 'b', 'g', 'h']\n",
      "1 ['a', 'e', 'c', 'f']\n",
      "1-gram V: 6\n",
      "2-gram V: 7\n",
      "3-gram V: 8\n",
      "4-gram V: 8\n"
     ]
    }
   ],
   "source": [
    "counts = Trie()\n",
    "\n",
    "# adding 4-grams\n",
    "counts.add(['a', 'b', 'c', 'd'])\n",
    "counts.add(['a', 'e', 'c', 'f'])\n",
    "counts.add(['a', 'b', 'g', 'h'])\n",
    "counts.add(['x', 'y', 'z', 'a'])\n",
    "\n",
    "# setting & getting meta-info\n",
    "counts.size = 4\n",
    "print('ngram size:', counts.size)\n",
    "\n",
    "# testing counts for n-grams of various sizes\n",
    "tests = [['a'], ['a', 'b'], ['a', 'x'], ['e', 'c', 'f'], ['a', 'e', 'c', 'f']]\n",
    "\n",
    "# getting counts\n",
    "for seq in tests:\n",
    "    print(counts.get(seq).count, seq)\n",
    "\n",
    "# traversing trie: getting all strings\n",
    "print(\"All In Trie:\")\n",
    "for seq in counts.traverse():\n",
    "    print(seq)\n",
    "\n",
    "# traversing trie: by prefix (['a'])\n",
    "print(\"In Trie for: {}\".format(['a']))\n",
    "for seq in counts.traverse(node=counts.get(['a']), sequence=['a']):\n",
    "    print(counts.get(seq).count, seq)\n",
    "    \n",
    "# getting size of ngram vocabulary\n",
    "for i in range(counts.size):\n",
    "    print(\"{}-gram V: {}\".format(i+1, counts.v(size=i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to extract ngrams (variable $n$) from a sequence (sentence) represented as a list\n",
    "- define a function to compute ngram counts from a corpus (list-of-lists) and store as a Trie\n",
    "- compute bigram counts for the training set of NL2SparQL4NLU\n",
    "- report `5` most frequent bigrams comparing to the reference values below (you can use `nbest` on `node.children`)\n",
    "\n",
    "\n",
    " word 1 | word 2 | count \n",
    ":-------|:-------|-------:\n",
    "show    | me     |   377\n",
    "the     | movie  |   267\n",
    "of      | the    |   186\n",
    "me      | the    |   122\n",
    "is      | the    |   120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Computing Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ \n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ \n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus (x could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "- define a function to compute ngram probabilities using ngram counts\n",
    "\n",
    "- compute probabilities of ngrams ($n=2$) in the training set of NL2SparQL4NLU\n",
    "- report probabilities of the following ngrams\n",
    "    - $p(the | of)$\n",
    "    - $p(the | is)$\n",
    "    - $p(play | the)$\n",
    "    - probabilities of all bigram where $w_1$ is \"*italy*\", i.e. $p(*|italy)$\n",
    "\n",
    "\n",
    "| ngram            | $$\\approx p$$ | $$\\approx\\log(p)$$ |\n",
    "|:-----------------|--------------:|-------------------:|\n",
    "| $$p(the|of)$$    | 0.31          | -1.18     \n",
    "| $$p(the|is)$$    | 0.36          | -1.03\n",
    "| $$p(play|the)$$  | 0.00          | \n",
    "| $$p(make|italy)$$| 0.50          |\n",
    "| $$p(in|italy)$$  | 0.50          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Underflow Problem\n",
    "\n",
    "Probabilities are usually small ($<1$).\n",
    "Multiplying many of those may cause *underflow* problem.\n",
    "\n",
    "Use the sum of the probabilities' logs instead of product\n",
    "\n",
    "| Properties     \n",
    "|:---------------\n",
    "| $$p(a) > p(b)$$\n",
    "| $$log(p(a)) > log(p(b))$$\n",
    "| $$log(a*b) = log(a) + log(b)$$\n",
    "| $$p(a) * p(b) \\rightarrow log(p(a)) + log(p(b))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "- update the function to compute probabilities to use log (use `math` library)\n",
    "- define a function to convert log probabilities to probabilities (`exp()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $p(w_{1},\\ldots ,w_{n})$ ($p(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Probability of a Sequence: [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities (chain rule). \n",
    "\n",
    "$$p(w_{1}^{n}) = p(w_1) p(w_2|w_1) p(w_3|w_1^2) ... p(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{p(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$p(w_i|w_{1}^{i-1}) \\approx p(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\n",
    "N-gram   | Equation                   |\n",
    ":--------|:---------------------------|\n",
    "unigram  | $$p(w_i)$$                 |\n",
    "bigram   | $$p(w_i|w_{i-1})$$         |\n",
    "trigram  | $$p(w_i|w_{i-2},w_{i-1})$$ |\n",
    "\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$p(w_{1}^{n}) = \\prod_{i=1}^{n}{p(w_i|w_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Sentence beginning & end tags\n",
    "\n",
    "Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence tag, on the other hand, makes the bigram model a true probability distribution (Jurafsky and Martin). \"Without it, the sentence probabilities for all sentences of a given length would sum to one. This model would define an infinite set of probability distributions, with one distribution per sentence length.\"\n",
    "\n",
    "For larger ngrams, we’ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__: \"The cat is fat\"\n",
    "\n",
    "$p($ `<s>`, the, cat, is, fat, `</s>` $) = p(the |$ `<s>` $) * p(cat | the) * p(is | cat) * p(fat | is) * p($ `</s>` $| fat)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "- define a function to add sentence beginning and end tags to a corpus as list-of-lists\n",
    "- define a function to compute sentence probability given an ngram model\n",
    "    - remember that for log we use sum for raw probabilities product\n",
    "        - use `math.prod` for Python 3.8\n",
    "        - use `numpy.prod` for Python < 3.8\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU\n",
    "- compute probability of sentences: *star of twilight* and *star of thor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = $ `<s>`;\n",
    "- *while* $w_i \\neq $ `</s>`\n",
    "\n",
    "    - stochastically get new word w.r.t. $p(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- define a function to generate random sentences (e.g. using `random.choice`)\n",
    "- generate `5` different sentences & score them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluation: [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\n",
    "$$ PPL = \\sqrt[N]{\\frac{1}{p(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}p(w_i|w_{i-N+1})}}$$\n",
    "\n",
    "Where $N$ is the number of words in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Handling Unseen Words\n",
    "\n",
    "Out-Of-Vocabulary (OOV) word -- tokens in test data that are not contained in the lexicon (vocabulary).\n",
    "Empirically each OOV word results in 1.5 - 2 extra errors (> 1 due to the loss of contextual information).\n",
    "\n",
    "__*How to handle words (in test set) that were never seen in the training data?*__\n",
    "\n",
    "Train a language model with specific token (e.g. `<unk>`) for unknown words!\n",
    "\n",
    "__*How to estimate probabilities of unknown words and ngrams?*__\n",
    "\n",
    "The *simplest* approach is to replace all the words that are not in vocabulary (lexicon) with the `<unk>` token and treat it as any other word. (For instance, applying frequency cut-off to the lexicon, will allow estimate these probabilities on the training set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- define a function to replace OOV words in a corpus as list-of-list given a lexicon\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU (with `<unk>`)\n",
    "- re-compute probability of sentences: *star of twilight* and *star of thor*\n",
    "    - replace unknown words and add tags as well\n",
    "- compare scores to the ones without unknown word handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Handling Data Sparseness\n",
    "\n",
    "What do we do with words that are in our lexicon, but appear in a test set in an unseen context (i.e. no ngram)?\n",
    "\n",
    "Similar to unseen words and unseen n-grams have $0$ probability; thus, whole sequence gets $0$ probability.\n",
    "(The problem is somewhat avoided using log probabilities.)\n",
    "\n",
    "Use smoothing:\n",
    "- Add some probability to unseen events\n",
    "- Remove some probability from seen events\n",
    "- Joint probability distribution sums to 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smoothing Methods\n",
    "\n",
    "Available smoothing methods: ([tutorial](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf))\n",
    "- [Additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (__simplest__)\n",
    "- Good-Turing estimate\n",
    "- Jelinek-Mercer smoothing (interpolation)\n",
    "- Katz smoothing (backoff)\n",
    "- Witten-Bell smoothing\n",
    "- Absolute discounting\n",
    "- Kneser-Ney smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1. Add-One Smoothing\n",
    "Kind of Additive Smoothing.\n",
    "\n",
    "__Bigrams__\n",
    "\n",
    "$V$ -- vocabulary size\n",
    "\n",
    "$$p(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$\n",
    "\n",
    "__N-grams__\n",
    "\n",
    "$V$ -- total number of possible $(N-1)$-grams\n",
    "\n",
    "$$p(w_i | w^{i-1}_{i-N+1}) = \\frac{c(w^{i-1}_{i-N+1},w_i)+1}{c(w^{i-1}_{i-N+1})+V}$$\n",
    "\n",
    "Typically, we assume $V = \\{w : c(w) > 0\\} \\cup \\{$ `<unk>` $\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- update the ngram probability calculation function to apply add one smoothing\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU with smoothing\n",
    "- compare scores to the ones without smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2. Interpolation (Jelinek-Mercer Smoothing)\n",
    "[Interpolation](https://en.wikipedia.org/wiki/Interpolation) estimates the probability by combining more robust, but weaker estimators; i.e. fall-back to lower order ngram probabilities to estimate a higher ngram probability. \n",
    "The standard way of combination is to use *linear interpolation*: to estimate a probability of a trigram, we use a weighted sum of unigrams, bigrams, and trigram probabilities as:\n",
    "\n",
    "$$p(w_i|w_{i-1}, w_{i-2}) = \\lambda_3 p(w_i|w_{i-1},w_{i-2}) + \\lambda_2 p(w_i|w_{i-1}) + \\lambda_1 p(w_i)$$\n",
    "\n",
    "Where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$.\n",
    "\n",
    "> For any size of ngram: $n^{th}$-order smoothed model is defined recursively as a linear interpolation between the $n^{th}$-order Maximum Likelihood (ML) model and the $(n−1)^{th}$-order smoothed model.\n",
    "\n",
    "$$p_{INT}(w_i|w_{i-N+1}^{i-1}) = \\lambda_{w_{i-N+1}^{i-1}} p_{ML}(w_i|w_{i-N+1}^{i-1}) + (1-\\lambda_{w_{i-N+1}^{i-1}}) p_{INT}(w_i|w_{i-N+2}^{i-1})$$\n",
    "\n",
    "The recursion can be grounded as:\n",
    "- unigram model: $p_{ML}(w_i)$\n",
    "- uniform distribution (e.g. for OOV)\n",
    "\n",
    "$$p_{U}(w_i)=\\frac{1}{V}$$\n",
    "\n",
    "Values of $\\lambda$s are computed using *__deleted interpolation__*: \n",
    "\n",
    "> we successively delete each trigram from the training corpus and choose the $\\lambda$s so as to maximize the likelihood of the rest of the corpus (similar to leave-one-out cross-validation). \n",
    "\n",
    "The deletion helps to set the $\\lambda$s in a way to prevent over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleted_interpolation(counts):\n",
    "    \"\"\"\n",
    "    generic deleted interpolation\n",
    "    :param counts: counts trie\n",
    "    :return: interpolation weights for ngram models\n",
    "    \"\"\"\n",
    "    w = [0] * counts.size\n",
    "    for ngram in counts.traverse():\n",
    "        # current ngram count\n",
    "        v = counts.get(ngram).count\n",
    "        # (n)-gram counts\n",
    "        n = [counts.get(ngram[0:i+1]).count for i in range(len(ngram))]\n",
    "        # (n-1)-gram counts -- parent node\n",
    "        p = [counts.get(ngram[0:i]).count for i in range(len(ngram))]\n",
    "        # -1 from both counts & normalize\n",
    "        d = [float((n[i]-1)/(p[i]-1)) if (p[i]-1 > 0) else 0.0 for i in range(len(n))]\n",
    "        # increment weight of the max by raw ngram count\n",
    "        k = d.index(max(d))\n",
    "        w[k] += v\n",
    "    return [float(v)/sum(w) for v in w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Putting it all together (Exercise)\n",
    "\n",
    "Train an Ngram Language Model (compute ngram probabilities) such that:\n",
    "\n",
    "- case insensitive (by default)\n",
    "- 2-gram\n",
    "- log probabilities\n",
    "- considers sentence boundaries (beginning and end of sentence tags)\n",
    "- considers unknown words\n",
    "- Add-One Smoothing\n",
    "\n",
    "Compute probabilities of utterances in `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
