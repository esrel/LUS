{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Labeling with Hidden Markov Models\n",
    "- Language Understanding Systems\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the Laboratory Work for [Language Understanding Systems class](http://disi.unitn.it/~riccardi/page7/page13/page13.html) of [University of Trento](https://www.unitn.it/en).\n",
    "Laboratory has been ported to jupyer notebook format for remote teaching during [COVID-19 pandemic](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/8.pdf) Chapter 8: Part-of-Speech Tagging (HMMs)\n",
    "- [NLTK](https://www.nltk.org/book/ch05.html) Chapter 5: Part of Speech Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements__\n",
    "\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [`conll.py`](https://github.com/esrel/LUS/) (in `src` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Labeling (Tagging)\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\n",
    "\n",
    "[Sequence Labeling](https://en.wikipedia.org/wiki/Sequence_labeling) is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. It is a sub-class of [structured (output) learning](https://en.wikipedia.org/wiki/Structured_prediction), since we are predicting a *sequence* object rather than a discrete or real value predicted in classification problems.\n",
    "\n",
    "- Can be treated as a set of independent classification tasks, one per member of the sequence;\n",
    "- Performance is generally improved by making the optimal label for a given element dependent on the choices of nearby elements;\n",
    "\n",
    "Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and [approximate inference](https://en.wikipedia.org/wiki/Approximate_inference) and learning methods are used. \n",
    "\n",
    "[Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) is a stochastic model used to describe sequences. It is the simplest [Markov Model](https://en.wikipedia.org/wiki/Markov_model). In order to make inference tractable, a process that generated the sequence is assumed to have [Markov Property](https://en.wikipedia.org/wiki/Markov_property), i.e. future states depend only on the current state, not on the events that occurred before it. (An [ngram](https://en.wikipedia.org/wiki/N-gram) [language model](https://en.wikipedia.org/wiki/Language_model) is a $(n-1)$-order Markov Model.) \n",
    "\n",
    "In Statical Language Modeling, we are modeling *observed sequences* represented as Markov Chains. Since the states of the process are *observable*, we only need to compute __transition probabilities__. \n",
    "\n",
    "In Sequence Labeling, we assume that *observed sequences* (__sentences__) have been generated by a Markov Process with *unobservable* (i.e. hidden) states (__labels__), i.e. [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model) (__HMM__). \n",
    "Since the states of the process are hidden and the output is observable, each state has a probability distribution over the possible output tokens, i.e. __emission probabilities__. \n",
    "\n",
    "Using these two probability distributions (__transition__ and __emission__), in sequence labeling, we are *inferring* the sequence of state transitions, given a sequence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Natural Language Processing (NLP) Tasks\n",
    "\n",
    "Below are some examples of NLP tasks that Sequence Labeling is applied to as one of the methods.\n",
    "\n",
    "The scenario when members of a sequence are mapped to higher order units (i.e. grouped together `[['a'],['b','c']]`) and assigned a category) is known as __shallow parsing__.\n",
    "\n",
    "- [Part-of-Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) (Chunking)\n",
    "    - [Phrase Chunking](https://en.wikipedia.org/wiki/Phrase_chunking)\n",
    "    - [Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) \n",
    "    - [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)\n",
    "    - Dependency [Parsing](https://en.wikipedia.org/wiki/Parsing) \n",
    "    - Discourse Parsing\n",
    "    - (Natural/Spoken) __Language Understanding__: Concept Tagging/Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Hidden Markov Model Tagging\n",
    "Tagging is one of the tasks [Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model) are used for.\n",
    "\n",
    "Given s word sequence $w_{1}^{n}$ the goal is to find the most probable tag sequence $t_{1}^{n}$. \n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} p(t_{1}^{n} | w_{1}^{n})$$\n",
    "\n",
    "We assume that a tag sequence has generated the given sequence of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using __Bayes's Rule__ \n",
    "\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Consequently, we compute:\n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}}\\frac{p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})}{p(w_{1}^{n})}$$\n",
    "\n",
    "Probability of a word sequence is the same for all tags, thus:\n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} p(w_{1}^{n} | t_{1}^{n})p(t_{1}^{n})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Parameter Learning\n",
    "The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of *state transition* and *emission probabilities*. The task is usually to derive the [*maximum likelihood estimate*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) of the parameters of the HMM given the set of output sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simplifying Assumptions\n",
    "\n",
    "- Probability of a word only depends on its own tag, not tags of other words in a sentence, thus:\n",
    "\n",
    "$$p(w_{1}^{n}|t_{1}^{n}) \\approx p(w_1|t_1)p(w_2|t_2) ... p(w_n|t_n)$$\n",
    "\n",
    "- Probability of a tag depends on previous N tags; i.e. Markov assumption (ngram), thus:\n",
    "\n",
    "$$p(t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i | t_{i-n+1}^{i-1})}$$\n",
    "\n",
    "- The (first-order) Markov assumption (bigram):\n",
    "\n",
    "$$p(t_{1}^{n}) \\approx p(t_1|t_0) p(t_2|t_1) ... p(t_n|t_{n-1})$$\n",
    "- or:\n",
    "$$p(t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i | t_{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.1.1. Estimating Transition Probabilities from Data\n",
    "\n",
    "- *Transition Probabilities* $p(t_i | t_{i-n+1}^{i-1})$ is an ngram model, and it is estimated using the same recipe we use for ngram language modeling; but using tag ngrams instead of word-ngrams. \n",
    "- It is assumed that the set of states is *finite* and known (i.e. there is no unknown (or OOV) state).\n",
    "- The same principles of *smoothing* apply for ngrams of state transitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculating Probability from Frequencies*\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(t_i) = \\frac{c(t_i)}{T}$$ \n",
    "Bigram   | $$p(t_i|t_{i-1}) = \\frac{c(t_{i-1},t_i)}{c(t_{i-1})}$$ \n",
    "Ngram    | $$p(t_i|t_{i-N+1}^{i-1}) = \\frac{c(t_{i-N+1}^{i-1}, t_i)}{c(t_{i-N+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $T$ is the total number of tags in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus ($x$ could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.1.2. Estimating Emission Probabilities from Data\n",
    "Similar to *transition probabilities*, *emission probabilities* can be estimated from annotated data counting relative frequencies of observations. Since we assume that probability of a word depends only on its tag, the equation is the following.\n",
    "\n",
    "$$p(w_i|t_i) = \\frac{c(t_i,w_i)}{c(t_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unknown Words* & *Unknown Word Models* \n",
    "\n",
    "Emission probabilities are subject to data sparseness; thus require handling unknown words. \n",
    "Consequently, we need to estimate probabilities for $p($ `<unk>` $|t_i)$. \n",
    "\n",
    "- We can assume that all tags ($t_i$) have equal probability of emitting `<unk>`; and estimate it as $\\frac{1}{V}$, where $V$ is the size of tag vocabulary.\n",
    "    - i.e. use Additive Smoothing\n",
    "- We can estimate them from data replacing OOV with `<unk>` and computing the probabilities\n",
    "- We can build __Unknown Word Model__ (like in Part-of-Speech Tagging), for instance using:\n",
    "    - word shape (capitalization)\n",
    "    - word class (word, punctuation, number)\n",
    "    - part-of-speech tags (generalize)\n",
    "    - word suffixes (last characters): e.g. suffixes of lengths (1 to 5) (e.g. [Samuelsson (1993)](https://www.aclweb.org/anthology/W93-0420.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Decoding\n",
    "$$t_{1}^{n} \\approx \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})$$\n",
    "\n",
    "| __Model__ | __Equation__ |\n",
    "|:----------|:--------------\n",
    "| *unigram* | $$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i)$$\n",
    "| *bigram*  | $$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-1})$$\n",
    "| *trigram* | $$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-2}, t_{i-1})$$\n",
    "\n",
    "where:\n",
    "- $p(w_i|t_i)$ -- *emission probability*, i.e. of seeing current word given the current tag\n",
    "- $p(t_i|t_{i-n+1}^{i-1})$ -- *transition probability*, i.e. of seeing the current tag given the tags we just saw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Viterbi Algorithm\n",
    "The decoding algorithm for HMMs is the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) -- an instance of dynamic programming. Bigram version of the algorithm is not difficult to implement (see pseudo-code in [SLP 8.4.5](https://web.stanford.edu/~jurafsky/slp3/8.pdf)); trigram, however, is more complex, and practical taggers incorporate other advanced features. \n",
    "\n",
    "There are numerous implementation available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Maximum Likelihood Estimation (__MLE__)\n",
    "\n",
    "Let's compare *emission probability* to *bigram probability* estimation computation:\n",
    "- Maximum Likelihood Estimation (__MLE__) from frequency counts\n",
    "- suffer from data sparseness:\n",
    "    - smoothing (__+1S__ - add-one smoothing, for simplicity)\n",
    "    - out-of-vocabulary (__OOV__, `<unk>`) word uniform probability estimation\n",
    "\n",
    "|         | __bigram *p*__ | __emission *p*__ |\n",
    "|:--------|:-----------------------|:-------------------------|\n",
    "| __MLE__ | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ | $$p(w_i|t_i) = \\frac{c(t_i,w_i)}{c(t_i)}$$\n",
    "| __+1S__ | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$ | $$p(w_i|t_i)=\\frac{c(t_i,w_i)+1}{c(t_i)+V}$$\n",
    "| __OOV__ | $$\\frac{1}{V}$$ | $$\\frac{1}{V}$$ \n",
    "\n",
    "In practice this means that we can estimate emission probabilities as ngram probabilities, i.e. using the same functions for counting and smoothing, treating $c(t_i,w_i)$ as $c(w_{i-1},w_i)$, i.e. as `[t_i, w_i]` ngram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shallow Parsing\n",
    "\n",
    "As we have already mentioned, [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) is a kind of Sequence Labeling. The main difference from Sequence Labeling task, such as Part-of-Speech tagging, where there is an output label (tag) per token; Shallow Parsing additionally performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "In other words, we want to be able to capture information that expressions like `\"New York\"` that consist of 2 tokens, constitute a single unit.\n",
    "\n",
    "What this means in practice is that Shallow Parsing performs *jointly* (or not) 2 tasks:\n",
    "- __Segmentation__ of input into constituents (__spans__)\n",
    "- __Classification__ (Categorization, Labeling) of these constituents into predefined set of labels (__types__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Revisiting Joint Probability Factorization\n",
    "In [*generative approach*](https://en.wikipedia.org/wiki/Generative_model) to Sequence Labeling we are modeling [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution).\n",
    "\n",
    "$$p(w_{1}^{n},t_{1}^{n}) = p(w_1, w_2, ..., w_n, t_1, t_2, ..., t_n)$$ \n",
    "\n",
    "To make the inference tractable, we factor the joint distribution using Chain Rule and apply [conditional independence assumption](https://en.wikipedia.org/wiki/Independence_(probability_theory)).\n",
    "\n",
    "$$P(A,B) = P(B|A)P(A) = P(A|B)P(B)$$\n",
    "\n",
    "It is common to mistakenly assume that $P(A|B) = P(B|A)$, known as [Confusion of the Inverse](https://en.wikipedia.org/wiki/Confusion_of_the_inverse).\n",
    "\n",
    "The relation between $P(A|B)$ and $P(B|A)$ is given by the Bayes Rule:\n",
    "\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Consequently: \n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) = \\frac{p(w_{1}^{n},t_{1}^{n})}{p(w_{1}^{n})} = \\frac{p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})}{p(w_{1}^{n})}$$\n",
    "\n",
    "If events $A$ and $B$ are conditionally independents, we have: \n",
    "\n",
    "$$P(A,B) = P(A)P(B) \\rightarrow P(A) = P(A|B); P(B) = P(B|A)$$\n",
    "\n",
    "Applying, Markov assumption to $p(t_{1}^{n})$ and conditional independence assumption to $p(w_{1}^{n} | t_{1}^{n})$ we end-up with our ngram sequence labeling model.\n",
    "\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would not apply conditional independence assumption to $p(w_{1}^{n}|t_{1}^{n})$, we would be modeling $p(w_{1}^{n},t_{1}^{n})$ __jointly__.\n",
    "\n",
    "Applying just Markov assumption, i.e. modeling it as an ngram (Markov Chain), we will be solving the following equation:\n",
    "\n",
    "$$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i},t_{i}|w_{i-N+1}^{i-1},t_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because:\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) = \\frac{p(w_{1}^{n},t_{1}^{n})}{p(w_{1}^{n})} = \\frac{p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})}{p(w_{1}^{n})}$$\n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max \\limits_{t_{1}^{n}} p(t_{1}^{n}|w_{1}^{n}) = \\arg\\max \\limits_{t_{1}^{n}} p(w_{1}^{n},t_{1}^{n}) = \\arg\\max \\limits_{t_{1}^{n}} p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})$$\n",
    "\n",
    "Most probable sequence can be obtained either way.\n",
    "\n",
    "$$t_{1}^{n} \\approx \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})$$ \n",
    "$$t_{1}^{n} \\approx \\arg\\max\\limits_{t_{1}^{n}} \\prod_{i=1}^{n}{p(w_{i},t_{i}|w_{i-N+1}^{i-1},t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "Factorization and conditional independence assumptions reduce computational complexity and requirements, and *amount of observations* needed to estimate model probabilities.\n",
    "\n",
    "Both models are applied to Shallow Parsing and Sequence Labeling in general:\n",
    "e.g. Hidden Markov Model Tagger and Stochastic Conceptual Language Models for Spoken Language Understanding in [Raymond & Riccardi (2007)](https://disi.unitn.it/~riccardi/papers2/IS07-GenerDiscrSLU.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Joint Segmentation and Classification\n",
    "In Shallow Parsing, the segmentation and label information is generally modeled *jointly*. \n",
    "In practice, it means that our output labels ($t_i$) can be decomposed into ($c_i,s_i$), where $c_i$ is classification label for token $i$, and $s_i$ segmentation label for token $i$.\n",
    "\n",
    "Consequently, in shallow parsing, we are modeling:\n",
    "\n",
    "$$p(w_{1}^{n},t_{1}^{n}) \\rightarrow p(w_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i,s_i)p(c_i,s_i|c_{i-N+1}^{i-1},s_{i-N+1}^{i-1})$$\n",
    "\n",
    "The joint modeling implies that we do not make conditional independence assumption between segmentation and classification label. However, assuming that probability of a words depends on segmentation and classification labels independently, while both depend on their previous N labels, we can factorize it as:\n",
    "\n",
    "$$p(w_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i)p(c_i|c_{i-N+1}^{i-1})p(w_i|s_i)p(s_i|s_{i-N+1}^{i-1})$$\n",
    "\n",
    "The *events* could be modeled independently as well: i.e. we can predict either classification labels only, or segmentation labels only.\n",
    "\n",
    "*Segmentation*:\n",
    "$$p(w_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|s_i)p(s_i|s_{i-N+1}^{i-1})$$\n",
    "*Classification*\n",
    "$$p(w_{1}^{n},c_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i)p(c_i|c_{i-N+1}^{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Join Modeling for Features\n",
    "In Shallow Parsing we jointly model *output label*.\n",
    "The principles of joint modeling could be applied to introduce additional features for *input tokens* as well. \n",
    "For instance, we could model jointly words and part-of-speech tags ($p_i$) for shallow parsing as:\n",
    "\n",
    "$$p(w_{1}^{n},p_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i,p_i|c_i,s_i)p(c_i,s_i|c_{i-N+1}^{i-1},s_{i-N+1}^{i-1})$$\n",
    "\n",
    "or predict them jointly with segmentation and classification labels as:\n",
    "\n",
    "$$p(w_{1}^{n},p_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i,s_i,p_i)p(c_i,s_i,p_i|c_{i-N+1}^{i-1},s_{i-N+1}^{i-1},p_{i-N+1}^{i-1})$$\n",
    "\n",
    "In the first case our input is *word-pos* pairs, we don't make independence assumptions, consequently they are treated as a single unit (i.e. you need to generate *pos* per word some other way for tagging). Same applies to *segmentation-classification* (or *segmentation-classification-pos*) output labels.\n",
    "\n",
    "- In joint modeling our observations for tokens and ngrams are more sparse: *word-pos* pair usually appears in data less than *word* and *pos* separately (same applies for their ngrams). \n",
    "- In joint modeling of output labels, we will have to estimate more of them, thus will have less observations for each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. [Bayesian Categorization](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "Assuming conditional independence between *word* and *pos* leads to Bayesian Categorization.\n",
    "\n",
    "$$p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{p(x)} = p(C_k) \\prod^n_{i=1} p(x_i|C_k)$$\n",
    "\n",
    "$$p(t_{i}|w_{i},p_{i}) \\approx p(t_{i})p(w_i|t_i)p(p_i|t_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Segmentation Information: CoNLL Corpus Format\n",
    "\n",
    "Corpus in CoNLL format consists of series of sentences, separated by blank lines. Each sentence is encoded using a table (or \"grid\") of values, where each line corresponds to a single word, and each column corresponds to an annotation type. \n",
    "\n",
    "The set of columns used by CoNLL-style files can vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "who    O\n",
    "plays  O\n",
    "luke   B-character.name\n",
    "on     O\n",
    "star   B-movie.name\n",
    "wars   I-movie.name\n",
    "new    I-movie.name\n",
    "hope   I-movie.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. [IOB Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "- The notation scheme is used to label *multi-word* spans in token-per-line format.\n",
    "    - *star wars new hope* is a *movie.name* concept that has 4 tokens\n",
    "- Both, prefix and suffix notations are commons: \n",
    "    - prefix: __B-movie.name__\n",
    "    - suffix: __movie.name-B__\n",
    "\n",
    "- Meaning of Prefixes\n",
    "    - __B__ for (__B__)eginning of span\n",
    "    - __I__ for (__I__)nside of span\n",
    "    - __O__ for (__O__)tside of span (no prefix or suffix, just `O`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Alternative Schemes:\n",
    "- No prefix or suffix (useful when there are no *multi-word* concepts)\n",
    "```\n",
    "        who    O\n",
    "        plays  O\n",
    "        luke   character.name\n",
    "        on     O\n",
    "        star   movie.name\n",
    "        wars   movie.name\n",
    "        new    movie.name\n",
    "        hope   movie.name\n",
    "```\n",
    "- __IOB/IOB2/BIO__\n",
    "\n",
    "- __IOBE__\n",
    "    - IOB + \n",
    "    - __E__ for (__E__)nd of span (or __L__ for (__L__)ast)\n",
    "```\n",
    "        who    O\n",
    "        plays  O\n",
    "        luke   B-character.name\n",
    "        on     O\n",
    "        star   B-movie.name\n",
    "        wars   I-movie.name\n",
    "        new    I-movie.name\n",
    "        hope   E-movie.name\n",
    "```\n",
    "    \n",
    "- __BILOU/BIOES__\n",
    "    - IOB + \n",
    "    - __L__ for (__L__)ast word of span\n",
    "    - __U__ for (__U__)nit word (or __S__ for (__S__)ingleton)\n",
    "```\n",
    "        who    O\n",
    "        plays  O\n",
    "        luke   U-character.name\n",
    "        on     O\n",
    "        star   B-movie.name\n",
    "        wars   I-movie.name\n",
    "        new    I-movie.name\n",
    "        hope   L-movie.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Choice of Scheme\n",
    "- It is possible to convert IOB, IOBE, & BILOU formats to each other\n",
    "- Each prefix is applied to every concept label, consequently we increase the number of transitions whose probabilities we need to estimate; \n",
    "    - increasing data sparseness, as for each label we will have less observations\n",
    "- The choice of scheme depends on the amount of available data:\n",
    "    - __IOB__ for least amount\n",
    "    - __BILOU__ for the most amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Terminology\n",
    "There is no strict naming convention regarding schemes (see alternatives) or how each constituent is termed. \n",
    "Below is the terminology used in this notebook. \n",
    "\n",
    "CoNLL is not the only data format for sequence labeling.\n",
    "\n",
    "Let's convert this representation into [rasa](https://rasa.com/) [markdown](https://rasa.com/docs/rasa/nlu/training-data-format/#markdown-format) and [json](https://rasa.com/docs/rasa/nlu/training-data-format/#json-format) formats, as a forward looking example, and explain terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CoNLL__\n",
    "```\n",
    "    who    O\n",
    "    plays  O\n",
    "    luke   B-character.name\n",
    "    on     O\n",
    "    star   B-movie.name\n",
    "    wars   I-movie.name\n",
    "    new    I-movie.name\n",
    "    hope   I-movie.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markdown__\n",
    "```markdown\n",
    "who plays [luke](character.name) on [star wars new hope](movie.name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__JSON__\n",
    "```json\n",
    "      {\n",
    "        \"intent\": \"actor\",\n",
    "        \"entities\": [\n",
    "          {\n",
    "            \"start\": 10,\n",
    "            \"end\": 14,\n",
    "            \"value\": \"luke\",\n",
    "            \"entity\": \"character.name\"\n",
    "          },\n",
    "          {\n",
    "            \"start\": 18,\n",
    "            \"end\": 36,\n",
    "            \"value\": \"star wars new hope\",\n",
    "            \"entity\": \"movie.name\"\n",
    "          }\n",
    "        ],\n",
    "        \"text\": \"who plays luke on star wars new hope\"\n",
    "      }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "All 3 formats encode the following information (ignoring some):\n",
    "- in string (sentence) `\"who plays luke on star wars new hope\"`\n",
    "- there are 2 __entities__ (a.k.a. concepts or slots, depending on NLP task and perspective), that have __types__ (labels)\n",
    "    - `character.name`\n",
    "    - `movie.name`\n",
    "    \n",
    "- entity of __type__ `movie.name`: \n",
    "    - has __span__:\n",
    "        - as tokens from `0` for *CoNLL*: `[5:7]`\n",
    "        - as string for *rasa md*: `\"star wars new hope\"`\n",
    "        - in characters from `0` for *rasa json*: `[18:36]`\n",
    "    - has __value__: `\"star wars new hope\"`\n",
    "        - string *covered by* (*on included*) in __span__\n",
    "        - __value__ could be *normalized* to a common value, e.g. `\"Star Wars: Episode IV - A New Hope\"` (not shown in examples)\n",
    " \n",
    "What *CoNLL* format additionally encodes is __tokenization__ informations. In other words, how string `\"star wars new hope\"` is split into tokens. Since most Sequence Labeling algorithms operate on token level, internally the strings are split into tokens, applying *IOB*-like schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Working with CoNLL Format Corpora: Exercise\n",
    "\n",
    "Using provided functions (adding your own), compute:\n",
    "- number of entity types (concepts) (implemented as an example as `get_chunks`)\n",
    "- number of output labels (i.e. `iob+type`)\n",
    "- frequency of each entity type in the NL2SparQL4NLU training set\n",
    "- frequency of each output label in the NL2SparQL4NLU training set\n",
    "- minimum, maximum, and average span sizes (in number of words) for each entity type and overall.\n",
    "- minimum, maximum, and average number of entities per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_corpus_conll(corpus_file, fs=\"\\t\"):\n",
    "    \"\"\"\n",
    "    read corpus in CoNLL format\n",
    "    :param corpus_file: corpus in conll format\n",
    "    :param fs: field separator\n",
    "    :return: corpus\n",
    "    \"\"\"\n",
    "    featn = None  # number of features for consistency check\n",
    "    sents = []  # list to hold words list sequences\n",
    "    words = []  # list to hold feature tuples\n",
    "\n",
    "    for line in open(corpus_file):\n",
    "        line = line.strip()\n",
    "        if len(line.strip()) > 0:\n",
    "            feats = tuple(line.strip().split(fs))\n",
    "            if not featn:\n",
    "                featn = len(feats)\n",
    "            elif featn != len(feats) and len(feats) != 0:\n",
    "                raise ValueError(\"Unexpected number of columns {} ({})\".format(len(feats), featn))\n",
    "            words.append(feats)\n",
    "        else:\n",
    "            if len(words) > 0:\n",
    "                sents.append(words)\n",
    "                words = []\n",
    "    return sents\n",
    "\n",
    "def parse_iob(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, None)\n",
    "\n",
    "def get_chunks(corpus_file, fs=\"\\t\", otag=\"O\"):\n",
    "    sents = read_corpus_conll(corpus_file, fs=fs)\n",
    "    return set([parse_iob(token[-1])[1] for sent in sents for token in sent if token[-1] != otag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise [Optional: Advanced]\n",
    "Implement bigram HMM\n",
    "- using Maximum Likelihood Estimation & smoothing compute:\n",
    "    - emission probabilities\n",
    "    - transition probabilities\n",
    "- implement Viterbi algorithm to work with estimated probabilities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence Labeling with NLTK\n",
    "[NLTK](https://www.nltk.org/api/nltk.tag.html) provides implementations of popular sequence labeling algorithms for Part-of-Speech Tagging (including [HMM](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)), that can be used for Sequence Labeling in general. \n",
    "\n",
    "- Loading & working with CoNLL format corpora in NLTK\n",
    "- Tagger training & testing (running)\n",
    "\n",
    "To have a custom tagger that labels input text with our __custom label set__, we need to __train__ it on a corpus annotated with this __custom label set__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. CoNLL format Corpus Loading\n",
    "\n",
    "NLTK provides method of working with CoNLL format corpora via [ConllCorpusReader](https://www.nltk.org/_modules/nltk/corpus/reader/conll.html).\n",
    "\n",
    "> The set of columns used by CoNLL-style files can vary from corpus to corpus; the `ConllCorpusReader` constructor therefore takes an argument, columntypes, which is used to specify the columns that are used by a given corpus. By default columns are split by consecutive whitespaces, with the separator argument you can set a string to split by (e.g. ' ')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConllChunkCorpusReader(root, fileids, chunk_types)` is a subclass of `ConllCorpusReader` for reading corpora consisting of 3 columns (it works with 2 columns (without pos-tags), as well):\n",
    "- words\n",
    "- part-of-speech tags\n",
    "- IOB-chunks (IOB-tagged labels)\n",
    "\n",
    "To load a corpus from file, the function requires us to provide\n",
    "- `root` - path to corpus\n",
    "- `fileids` - pattern to use to read files (filename also works)\n",
    "- `chunk_types` - set of *IOB-tag stripped labels* excluding out-of-span tag `'O'`\n",
    "    - output of `get_chunks` above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actor.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor.nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actor.type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>award.category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>award.ceremony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>character.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>country.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>director.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>director.nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>movie.description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>movie.genre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>movie.gross_revenue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>movie.language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>movie.location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>movie.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>movie.release_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie.release_region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>movie.star_rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>movie.subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>person.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>person.nationality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>producer.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rating.name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "0             actor.name\n",
       "1      actor.nationality\n",
       "2             actor.type\n",
       "3         award.category\n",
       "4         award.ceremony\n",
       "5         character.name\n",
       "6           country.name\n",
       "7          director.name\n",
       "8   director.nationality\n",
       "9      movie.description\n",
       "10           movie.genre\n",
       "11   movie.gross_revenue\n",
       "12        movie.language\n",
       "13        movie.location\n",
       "14            movie.name\n",
       "15    movie.release_date\n",
       "16  movie.release_region\n",
       "17     movie.star_rating\n",
       "18         movie.subject\n",
       "19           person.name\n",
       "20    person.nationality\n",
       "21         producer.name\n",
       "22           rating.name"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader.conll import ConllChunkCorpusReader\n",
    "import nltk.tag.hmm as hmm\n",
    "import pandas as pd\n",
    "\n",
    "trn='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.conll.txt'\n",
    "\n",
    "# reading our corpus (nltk requires us to provide chunk labels, i.e. concepts)\n",
    "concepts = sorted(get_chunks(trn))\n",
    "\n",
    "# loading training & test sets providing\n",
    "# - path\n",
    "# - file name pattern (to allow reading multiple files)\n",
    "# - chunk label set\n",
    "trn_data = ConllChunkCorpusReader('NL2SparQL4NLU/dataset/',  r'NL2SparQL4NLU.train.conll.txt', concepts)\n",
    "tst_data = ConllChunkCorpusReader('NL2SparQL4NLU/dataset/',  r'NL2SparQL4NLU.test.conll.txt', concepts)\n",
    "   \n",
    "pd.DataFrame(concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Common Data Access Methods\n",
    "\n",
    "If the corpus has a relevant annotation, methods below could be used to access it after loading.\n",
    "\n",
    "| __Method__                 | __Returns__ |\n",
    "|:---------------------------|:------------|\n",
    "| `I{corpus}.words()`        | list of str\n",
    "| `I{corpus}.sents()`        | list of (list of str)\n",
    "| `I{corpus}.paras()`        | list of (list of (list of str))\n",
    "| `I{corpus}.tagged_words()` | list of (str,str) tuple\n",
    "| `I{corpus}.tagged_sents()` | list of (list of (str,str))\n",
    "| `I{corpus}.tagged_paras()` | list of (list of (list of (str,str)))\n",
    "| `I{corpus}.chunked_sents()`| list of (Tree w/ (str,str) leaves)\n",
    "| `I{corpus}.parsed_sents()` | list of (Tree with str leaves)\n",
    "| `I{corpus}.parsed_paras()` | list of (list of (Tree with str leaves))\n",
    "| `I{corpus}.xml()`          | A single xml ElementTree\n",
    "| `I{corpus}.raw()`          | str (unprocessed corpus contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: ['star', 'of', 'thor']\n",
      "TAG  : [('star', 'O'), ('of', 'O'), ('thor', 'B-movie.name')]\n",
      "PATH : ['O', 'O', 'B-movie.name']\n",
      "Accuracy: 0.9087\n"
     ]
    }
   ],
   "source": [
    "# training hmm on training data\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_model.train(trn_data.tagged_sents())\n",
    "\n",
    "# tagging sentences in test set\n",
    "for s in tst_data.sents():\n",
    "    print(\"INPUT: {}\".format(s))\n",
    "    print(\"TAG  : {}\".format(hmm_tagger.tag(s)))\n",
    "    print(\"PATH : {}\".format(hmm_tagger.best_path(s)))\n",
    "    break\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_tagger.evaluate(tst_data.tagged_sents())\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Tagging with NLTK\n",
    "- Experiment with different taggers provided in NLTK (e.g. NgramTagger)\n",
    "- Explore and experiment with different tagger parameters\n",
    "    - some of them have *cut-off*\n",
    "- For each report evaluation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Segmentation \n",
    "- Strip concept information from output labels\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Why do we want to evaluate a system / an algorithm's performance?*\n",
    "    - To measure one or more of its qualities.\n",
    "    - Proper evaluation criteria is a way to specify the problem.\n",
    "- *How do we evaluate a system / an algorithms performance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1. Automatic vs. Manual Evaluation\n",
    "- Automatic Evaluation (__OBJECTIVE__)\n",
    "    - Compare the system's output with the gold standard (reference)\n",
    "        - *Cons*: An effort to produce the gold standard (manual)\n",
    "        - *Pros*: Re-usable; no additional cost\n",
    "- Manual Evaluation (__SUBJECTIVE__)\n",
    "    - Ask human judges to estimate the quality w.r.t. certain criteria\n",
    "        - For some tasks the gold standard might be unobtainable\n",
    "        - No agreed automatic evaluation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Intrinsic vs. Extrinsic Evaluation\n",
    "- Intrinsic\n",
    "    - in isolation\n",
    "    - w.r.t. gold standard (references)\n",
    "    - e.g. Tagging performance\n",
    "- Extrinsic\n",
    "    - as a part of other system\n",
    "    - usefulness for some other task\n",
    "    - e.g. effect of POS-Tagger on parsing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3. Black-Box vs. Glass-Box Evaluation\n",
    "- Black-Box\n",
    "    - Evaluation of Performance\n",
    "        - speed\n",
    "        - accuracy \n",
    "        - etc.\n",
    "- Glass-Box\n",
    "    - Evaluation of Design algorithm\n",
    "        - used resources \n",
    "        - etc.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4. Gold Standard / References\n",
    "- *Where Gold Standard comes from?*\n",
    "    - Annotation by experts (human judges)\n",
    "- *How do we know that Gold Standard is good?*\n",
    "    - Evaluate agreement between the annotators/judges\n",
    "    - Most simple agreement measure: % of agreed instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5. Lower & Upper Bounds of the Performance\n",
    "- Lower Bound: __Baseline__ \n",
    "    - trivial solution to the problem: \n",
    "        - *random*: random decision\n",
    "        - *chance*: random decision w.r.t. the distribution of categories in the training data\n",
    "        - *majority*: assign everything to the largest category \n",
    "        - etc.\n",
    "- Upper Bound\n",
    "    - Inter-rater agreement, i.e. human performance.\n",
    "\n",
    "__A system is expected to perform within the lower and upper bounds.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5. Data Split\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Accuracy: Simplest Case\n",
    "\n",
    "$$ Accuracy = \\frac{\\text{Num. of Correct Decisions}}{\\text{Total Num. of Testing Instances}}$$\n",
    "\n",
    "- Known number of instances\n",
    "- Single decision for each instance\n",
    "- Single correct answer for each instance\n",
    "- All errors are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. [Contingency Table](https://en.wikipedia.org/wiki/Contingency_table)\n",
    "Contingency table represents frequencies of correct and wrong decisions by our system, and is useful to describe evaluation metrics.\n",
    "Hypotheses (__HYP__) on rows & references (__REF__) on columns.\n",
    "\n",
    "\n",
    "|         | __POS__ | __NEG__ |\n",
    "|---------|---------|---------|\n",
    "| __POS__ | TP      | FP      |\n",
    "| __NEG__ | FN      | TN      |\n",
    "\n",
    "Notation:\n",
    "\n",
    "|        |      |                | *Description*      |          |\n",
    "|--------|------|:---------------|--------------------|:---------|\n",
    "| __TN__ | *a*  | True Positive  | `HYP: + && REF: +` | correct\n",
    "| __FP__ | *b*  | False Positive | `HYP: + && REF: -` | error \n",
    "| __FN__ | *c*  | False Negative | `HYP: - && REF: +` | error \n",
    "| __TN__ | *d*  | True Negative  | `HYP: - && REF: -` | correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy__:\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}$$\n",
    "\n",
    "- What if __TN__ is infinite or unknown? (e.g.: Number of irrelevant queries to a search engine)\n",
    "\n",
    "__Precision & Recall__\n",
    "\n",
    "$$Precison = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "- 2 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__F-Measure__: precision-recall trade-off\n",
    "\n",
    "- Harmonic Mean of Precision & Recall \n",
    "- Usually evenly weighted \n",
    "    - $\\beta = 1$ is $F_{1}$-measure\n",
    "    \n",
    "$$F_\\beta = \\frac{(1 +\\beta^2)*Precision * Recall}{\\beta^2*Precision + Recall}$$\n",
    "\n",
    "$$F_1= \\frac{2*Precision * Recall}{Precision + Recall}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_{1}$-measure is better for evaluation of Shallow Parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Other Topics in Evaluation\n",
    "- Edit Distance (Error Rate)\n",
    "- Cross-Validation\n",
    "- Significance Tests\n",
    "- Agreement Measures\n",
    "- Sampling (random, stratified)\n",
    "- Binary vs. Multi-class classification\n",
    "- Multi-label data\n",
    "- Regression\n",
    "- Re-ranking\n",
    "- Ensemble Methods\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Evaluation Metrics\n",
    "Using references and prediction of the HMM-tagger on `NL2SparQL4NLU` dataset\n",
    "- compute raw TP, FP, FN, TN (on `iob+type`)\n",
    "- implement evaluation metrics & report:\n",
    "\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - $F_{1}$-Measure\n",
    " \n",
    "- compare accuracy to output of NLTK\n",
    "\n",
    "You can collect predictions as:\n",
    "```python\n",
    "hyps = [hmm_tagger.tag(s) for s in tst_data.sents()]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. CoNLL Eval: Exercise\n",
    "CoNLL Community developed a perl script to evaluate *segmentation* and *labeling* performance jointly using IOB information. Such evaluation provides more accurate assessment of the shallow parsing performance, in comparison to token-level metrics (e.g. NLTK accuracy).\n",
    "\n",
    "- import `evaluate` function from `conll.py` (example shown)\n",
    "- evaluate tagger predictions\n",
    "- compare performances to token-level accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>director.name</th>\n",
       "      <td>0.584</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.570</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.star_rating</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.subject</th>\n",
       "      <td>0.737</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.683</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_date</th>\n",
       "      <td>0.741</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.714</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.name</th>\n",
       "      <td>0.853</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.797</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.category</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.release_region</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.333</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.type</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character.name</th>\n",
       "      <td>0.583</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.519</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>producer.name</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.726</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating.name</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.941</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.location</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.nationality</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.type</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor.name</th>\n",
       "      <td>0.692</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.684</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person.name</th>\n",
       "      <td>0.324</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.324</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director.nationality</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award.ceremony</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.400</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country.name</th>\n",
       "      <td>0.593</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.579</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.gross_revenue</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.750</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.language</th>\n",
       "      <td>0.732</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.656</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie.genre</th>\n",
       "      <td>0.897</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.717</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          p      r      f     s\n",
       "director.name         0.584  0.556  0.570    81\n",
       "movie.star_rating     1.000  0.000  0.000     1\n",
       "movie.subject         0.737  0.636  0.683    44\n",
       "movie.release_date    0.741  0.690  0.714    29\n",
       "movie.name            0.853  0.748  0.797   473\n",
       "award.category        1.000  0.000  0.000     2\n",
       "movie.release_region  0.500  0.250  0.333     4\n",
       "movie.type            1.000  0.000  0.000     4\n",
       "character.name        0.583  0.467  0.519    15\n",
       "producer.name         0.882  0.616  0.726    73\n",
       "rating.name           0.966  0.918  0.941    61\n",
       "movie.location        0.333  0.143  0.200     7\n",
       "actor.nationality     1.000  0.000  0.000     1\n",
       "actor.type            1.000  1.000  1.000     2\n",
       "actor.name            0.692  0.675  0.684    80\n",
       "person.name           0.324  0.324  0.324    34\n",
       "director.nationality  1.000  0.000  0.000     1\n",
       "award.ceremony        0.667  0.286  0.400     7\n",
       "country.name          0.593  0.565  0.579    62\n",
       "movie.gross_revenue   1.000  0.600  0.750     5\n",
       "movie.language        0.732  0.594  0.656    69\n",
       "movie.genre           0.897  0.722  0.800    36\n",
       "total                 0.772  0.670  0.717  1091"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd\n",
    "\n",
    "# getting references\n",
    "refs = [s for s in tst_data.tagged_sents()]\n",
    "# getting hypotheses\n",
    "hyps = [hmm_tagger.tag(s) for s in tst_data.sents()]\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. OOV: Exercise\n",
    "\n",
    "- extend unknown word handling function to CoNLL format\n",
    "- train and evaluate tagger on OOV-processed training data\n",
    "- compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
