{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Modeling\n",
    "\n",
    "- Language Understanding Systems\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is part of the Laboratory Work for [Language Understanding Systems class](http://disi.unitn.it/~riccardi/page7/page13/page13.html) of [University of Trento](https://www.unitn.it/en).\n",
    "Laboratory has been ported to jupyer notebook format for remote teaching during [COVID-19 pandemic](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic).\n",
    "\n",
    "This notebook covers Lecture on __Sequence and Language Modeling__.\n",
    "\n",
    "Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/)) is adviced for reading. \n",
    "\n",
    "- Section *Corpora and Counting* covers some conceptes of *Chapter 2: \"Regular Expressions, Text Normalization, Edit Distance\"*.\n",
    "- Sections *Ngrams and Ngram Probabilites* and *Language Models* roughly cover *Chapter 3: \"N-gram Language Models\"*. \n",
    "\n",
    "<span style=\"color: blue;\">__NOTE__</span>: \n",
    "\n",
    "Due to popularity of Python as a programming language for Natural Langauge Processing and Machine Learning, the exercises use Python (occassionally Unix Shell).\n",
    "Prefer to avoid using libraries, unless exlicitly stated (e.g. don't use [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset\n",
    "\n",
    "    - run `git clone https://github.com/esrel/NL2SparQL4NLU.git`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL2SparQL4NLU\n",
    "\n",
    "- __Format__:\n",
    "\n",
    "    - Utterance (sentence) per line\n",
    "    - Tokenized\n",
    "    - Lowercased\n",
    "\n",
    "- __Language__: English monolingual\n",
    "\n",
    "- __Annotation__: None (for now)\n",
    "\n",
    "- __Split__: training & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to load a corpus into a list-of-lists\n",
    "\n",
    "- load `NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt`\n",
    "- print first `2` words (tokens) of the first `10` sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
    "tst='NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* description in terms of:\n",
    "\n",
    "- total number of words\n",
    "- total number of utterances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute corpus descriptive statistics -- total utterance and word counts.\n",
    "- compute the statistics for the __training__ and __test__ sets of NL2SparQL4NLU dataset. \n",
    "- compare the computed statistics with the reference values below.\n",
    "\n",
    "\n",
    "| Metric           | Train  | Test   |\n",
    "|------------------|-------:|-------:|\n",
    "| Total Words      | 21,453 |  7,117 |\n",
    "| Total Utterances |  3,338 |  1,084 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexion (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute a lexicon from corpus in a list-of-lists format\n",
    "    - sort the list alphabetically\n",
    "    \n",
    "- compute the lexicon of the training set of NL2SparQL4NLU dataset\n",
    "- compare the its size to the reference value below.\n",
    "\n",
    "| Metric       | Value |\n",
    "|--------------|------:|\n",
    "| Lexicon Size | 1,729 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts (our corpus is lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute a frequency list for a corpus\n",
    "- compute frequency list for the training set of NL2SparQL4NLU dataset\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compate the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| the    |     1,337 |\n",
    "| movies |     1,126 |\n",
    "| of     |       607 |\n",
    "| in     |       582 |\n",
    "| movie  |       564 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-values\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*).\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Cut-Off\n",
    "\n",
    "- define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "\n",
    "    - use default values for min and max\n",
    "    \n",
    "- using frequency list for the training set of NL2SparQL4NLU dataset\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 1729 |\n",
    "| cut-off    |   2 | N/A |  950 |\n",
    "| cut-off    | N/A | 100 | 1694 |\n",
    "| cut-off    |   2 | 100 |  915 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop Word Removal\n",
    "\n",
    "- define a function to read/load a list of words in token-per-line format (i.e. lexicon)\n",
    "- load stop word list from `NL2SparQL4NLU/extras/english.stop.txt`\n",
    "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    \n",
    "    - define a function to compute overlap of two lexicons\n",
    "    - define a function to apply a stopword list to a lexicon\n",
    "\n",
    "- compare the 100 most frequent words in frequency list of the training set to the list of stopwords (report count)\n",
    "- apply stopword list to the lexicon of the training set\n",
    "- report size of the resulting lexicon comparing to the reference values.\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 1729 |\n",
    "| no stop words   | 1529 |\n",
    "| top 100 overlap |   50 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swl='NL2SparQL4NLU/extras/english.stop.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: blue;\">Solutions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    read corpus into a list-of-lists, splitting sentences into tokens by space (' ')\n",
    "    :param corpus_file: corpus file in sentence-per-line format (tokenized)\n",
    "    :return: corpus as list of lists\n",
    "    \"\"\"\n",
    "    return [line.strip().split() for line in open(corpus_file, 'r')]\n",
    "\n",
    "def read_lexicon(lexicon_file):\n",
    "    \"\"\"\n",
    "    read lexicon into a list\n",
    "    :param lexicon_file: lexicon file in token-per-line format\n",
    "    :return: lexicon as a list\n",
    "    \"\"\"\n",
    "    return [line.strip() for line in open(lexicon_file, 'r')]\n",
    "\n",
    "def compute_lexicon(corpus):\n",
    "    \"\"\"\n",
    "    compute lexicon of a corpus\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :return: sorted list of unique words\n",
    "    \"\"\"\n",
    "    return sorted(list(set([word for sent in corpus for word in sent])))\n",
    "\n",
    "def compute_frequency_list(corpus):\n",
    "    \"\"\"\n",
    "    create frequency list for a corpus\n",
    "    :param corpus: corpus in list-of-lists format\n",
    "    :return: frequency list as dict of counts\n",
    "    \"\"\"\n",
    "    frequencies = {}\n",
    "    for sentence in corpus:\n",
    "        for token in sentence:\n",
    "            frequencies[token] = frequencies.setdefault(token, 0) + 1 \n",
    "    return frequencies\n",
    "\n",
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n top values from a dict\n",
    "    :param d: input dict (values are numbers)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-values\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "def cutoff(frequency_list, tf_min=1, tf_max=float('inf')):\n",
    "    \"\"\"\n",
    "    apply min and max cutoffs to a frequency list\n",
    "    :param frequency_list: frequency list of a corpus as dict\n",
    "    :param tf_min: minimum token frequency for lexicon elements (below removed); default 1\n",
    "    :param tf_max: maximum token frequency for lexicon elements (above removed); default infinity\n",
    "    :return: lexicon as sorted list of tokens\n",
    "    \"\"\"\n",
    "    return sorted([token for token, frequency in frequency_list.items() if tf_max >= frequency >= tf_min])\n",
    "\n",
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    compute overal of two lists as set intersection\n",
    "    :param a: list 1\n",
    "    :param b: list 2\n",
    "    :return: sorted list of overlapping elements\n",
    "    \"\"\"\n",
    "    return sorted(list(set(a) & set(b)))\n",
    "    \n",
    "def remove_stopwords(lexicon, stopwords):\n",
    "    \"\"\"\n",
    "    remove stopwords from a lexicon\n",
    "    :param lexicon: lexicon as a list\n",
    "    :param stopwords: stopwords list\n",
    "    :return: sorted difference of two lists (lexicon - stopwords)\n",
    "    \"\"\"\n",
    "    return sorted(list(set(lexicon) - set(stopwords)))\n",
    "\n",
    "def corpus_stats(corpus):\n",
    "    \"\"\"\n",
    "    compute word and sentence counts of the corpus\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :return: sentence count, word count\n",
    "    \"\"\"\n",
    "    return len(corpus), sum([len(sent) for sent in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths to data set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = 'NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
    "tst = 'NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'\n",
    "swl = 'NL2SparQL4NLU/extras/english.stop.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Reading & Iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'plays']\n",
      "['show', 'credits']\n",
      "['who', 'was']\n",
      "['find', 'the']\n",
      "['who', 'played']\n",
      "['who', 'was']\n",
      "['who', 'played']\n",
      "['who', 'was']\n",
      "['find', 'the']\n",
      "['cast', 'and']\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus(trn)\n",
    "\n",
    "for sentence in corpus[:10]:\n",
    "    print(sentence[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3338, 21453)\n",
      "(1084, 7117)\n"
     ]
    }
   ],
   "source": [
    "print(corpus_stats(read_corpus(trn)))\n",
    "print(corpus_stats(read_corpus(tst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Lexicon & Its Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1729\n"
     ]
    }
   ],
   "source": [
    "lex = compute_lexicon(corpus)\n",
    "print(len(lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1337, 'movies': 1126, 'of': 607, 'in': 582, 'movie': 564}\n"
     ]
    }
   ],
   "source": [
    "freq_list = compute_frequency_list(corpus)\n",
    "print(nbest(freq_list, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Cut-Off & Reporting Lexicon Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1729\n",
      "950\n",
      "1694\n",
      "915\n"
     ]
    }
   ],
   "source": [
    "print(len(cutoff(freq_list)))\n",
    "print(len(cutoff(freq_list, tf_min=2)))\n",
    "print(len(cutoff(freq_list, tf_max=100)))\n",
    "print(len(cutoff(freq_list, tf_min=2, tf_max=100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Lexicon Overlap (Set Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "stopwords = read_lexicon(swl)\n",
    "top_words = list(nbest(freq_list, n=100).keys())\n",
    "print(len(compute_overlap(top_words, stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stopwords (Set Difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529\n"
     ]
    }
   ],
   "source": [
    "print(len(remove_stopwords(lex, stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ngrams and Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of *n* items from a given sequence of text or speech. An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character n-grams: cat\n",
    "- word n-grams: the cat is fat\n",
    "\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['c', 'a', 't']` | `['ca', 'at']` | `['cat']` |\n",
    "| *Word N-grams*      | `['the', 'cat', 'is' , 'fat']` | `['the cat', 'cat is', ...]` | `['the cat is', ...]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Counting Ngrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count. Ngram count only differs in a unit of counting -- sequence of $n$ of words. We can compute bigram count by taking sequences of 2 items, trigrams - 3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Data Structures\n",
    "For *Frequency List* we have used [`dict`](https://docs.python.org/2/library/stdtypes.html#dict) to store counts.\n",
    "In ngram counting scenario we still can use dictionary, and use ngram string keys; however, there are better data structures for the task. The frequent data structures used to store ngram counts or probabilities are [hash table](https://en.wikipedia.org/wiki/Hash_table), [trie](https://en.wikipedia.org/wiki/Trie), or [finite state automaton](https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton). The pros and cons of each data structure are out of the scope of this lab; for the discussion on efficient data structures for language modeling please refer to [KenLM paper](https://kheafield.com/papers/avenue/kenlm.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Implementing Trie\n",
    "There are plenty of implementation of the Trie data structure (e.g. [`pygtrie`](https://github.com/google/pygtrie/) from Google). However, it is simple enough. \n",
    "\n",
    "For understanding purposes let's implement our own, such that:\n",
    "\n",
    "- works on lists of words\n",
    "- stores in a node word, count, and children (`Node` class)\n",
    "- implements methods to:\n",
    "    - add a sequence (updating counts)\n",
    "    - get a node by a sequence (i.e. prefix)\n",
    "    - set a node attribute given sequence, key and value\n",
    "    - traverse a trie and get all sequences\n",
    "    \n",
    "It is convenient to introduce an `error` node to the Trie to easily handle sequences not in it.\n",
    "Thus, we also add `self.error` with `count` set to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "    def __init__(self, word=None):\n",
    "        self.word = word\n",
    "        self.children = {}\n",
    "        self.count = 0\n",
    "        \n",
    "    def __set__(self, instance, value):\n",
    "        self.instance = value\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        return self.instance\n",
    "\n",
    "\n",
    "class Trie(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.root = Node('*')\n",
    "        self.error = Node()\n",
    "    \n",
    "    # convenient to set and get values (e.g. model metadata)\n",
    "    def __set__(self, instance, value):\n",
    "        self.instance = value\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        return self.instance\n",
    "\n",
    "    def add(self, sequence):\n",
    "        node = self.root\n",
    "        node.count += 1  # total count\n",
    "        for word in sequence:\n",
    "            node.children[word] = node.children.setdefault(word, Node(word))\n",
    "            node = node.children[word]\n",
    "            node.count += 1\n",
    "\n",
    "    def get(self, sequence):\n",
    "        node = self.root\n",
    "        for word in sequence:\n",
    "            if not node.children:\n",
    "                return self.error\n",
    "            elif not word in node.children:\n",
    "                return self.error\n",
    "            else:\n",
    "                node = node.children.get(word)\n",
    "        return node\n",
    "    \n",
    "    def traverse(self, node=None, sequence=[]):\n",
    "        node = self.root if not node else node\n",
    "        \n",
    "        if not node.children:\n",
    "            yield sequence\n",
    "\n",
    "        for word, n in node.children.items():\n",
    "            sequence.append(word)\n",
    "            yield from self.traverse(n, sequence)\n",
    "            sequence.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing out the implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n",
      "['a', 'b', 'c', 'd']\n",
      "['a', 'e', 'c', 'f']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "counts = Trie()\n",
    "counts.add(['a', 'b', 'c', 'd'])\n",
    "counts.add(['a', 'e', 'c', 'f'])\n",
    "\n",
    "counts.size = 4\n",
    "\n",
    "tests = [['a'], ['a', 'b'], ['a', 'x']]\n",
    "\n",
    "for seq in tests:\n",
    "    print(counts.get(seq).count)\n",
    "\n",
    "for seq in counts.traverse():\n",
    "    print(seq)\n",
    "    \n",
    "print(counts.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to extract ngrams (variable $n$) from a sequence (sentence) represented as a list\n",
    "- define a function to compute ngram counts from a corpus (list-of-lists) and store as a Trie\n",
    "- compute bigram counts for the training set of NL2SparQL4NLU\n",
    "- report `5` most frequent bigrams comparing to the reference values below (you can use `nbest` on `node.children`)\n",
    "\n",
    "\n",
    " word 1 | word 2 | count \n",
    ":-------|:-------|-------:\n",
    "show    | me     |   377\n",
    "the     | movie  |   267\n",
    "of      | the    |   186\n",
    "me      | the    |   122\n",
    "is      | the    |   120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Computing Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ \n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ \n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurences of $x$ in a corpus (x could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "- define a function to compute ngram probabilities using ngram counts\n",
    "\n",
    "- compute probabilities of ngrams ($n=2$) in the training set of NL2SparQL4NLU\n",
    "- report probabilities of the following ngrams\n",
    "    - $p(the | of)$\n",
    "    - $p(the | is)$\n",
    "    - $p(play | the)$\n",
    "    - probabilities of all bigram where $w_1$ is \"*italy*\", i.e. $p(*|italy)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Underflow Problem\n",
    "\n",
    "Probabilities are usually small ($<1$).\n",
    "Multiplying many of those may cause *underflow* problem.\n",
    "\n",
    "Use the sum of the probabilities' logs instead of product\n",
    "\n",
    "| Properties     \n",
    "|:---------------\n",
    "| $$p(a) > p(b)$$\n",
    "| $$log(p(a)) > log(p(b))$$\n",
    "| $$log(a*b) = log(a) + log(b)$$\n",
    "| $$p(a) * p(b) \\rightarrow log(p(a)) + log(p(b))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "- update the function to compute probabilities to use log (use `math` library)\n",
    "- define a function to convert log probabilities to probabilities (`exp()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: blue;\">Solutions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(sequence, n=2):\n",
    "    \"\"\"\n",
    "    returns ngrams as a list-of-lists of sequence elements\n",
    "    :param sequence: list of elements\n",
    "    :param n: ngram size to extract\n",
    "    :return: list of ngrams\n",
    "    \"\"\"\n",
    "    return [sequence[i:i+n] for i in range(len(sequence) - n + 1)]\n",
    "\n",
    "def ngramcount(corpus, n=2):\n",
    "    \"\"\"\n",
    "    count ngrams in a corpus and stores as a Trie\n",
    "    :param corpus: list-of-lists\n",
    "    :param n: ngram size to count\n",
    "    :param glue: symbol for ngram concatenation into string\n",
    "    :return: dict of ngram frequencies\n",
    "    \"\"\"\n",
    "    counts = Trie()\n",
    "    counts.size = n  # meta-info: ngram-size\n",
    "    for sent in corpus:\n",
    "        for ngram in ngrams(sent, n=n):\n",
    "            counts.add(ngram) \n",
    "    return counts\n",
    "\n",
    "def ngramprobs(counts, logs=True):\n",
    "    \"\"\"\n",
    "    compute ngram probabilities from frequency counts\n",
    "    :param counts: counts trie\n",
    "    :param logs: if to compute log-probabilities\n",
    "    :return: trie augmented with probabilties\n",
    "    \"\"\"\n",
    "    from math import log\n",
    "    \n",
    "    # update default values:\n",
    "    counts.error.probability = 0.0  # set error node probability to 0\n",
    "    counts.logs = logs  # meta-info: log probabilities are used\n",
    "    \n",
    "    for ngram in counts.traverse(): \n",
    "        n = counts.get(ngram)       # get ngram node\n",
    "        p = counts.get(ngram[:-1])  # get parent node\n",
    "        prob = n.count / p.count\n",
    "        n.probability = log(prob) if logs else prob\n",
    "    return counts\n",
    "\n",
    "def logp2p(value):\n",
    "    from math import exp\n",
    "    return exp(value) if value else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting ngrams & Reporting most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'show+me': 377, 'the+movie': 267, 'of+the': 186, 'me+the': 122, 'is+the': 120}\n"
     ]
    }
   ],
   "source": [
    "counts = ngramcount(corpus, n=2)\n",
    "\n",
    "print(nbest({\"+\".join(ngram): counts.get(ngram).count for ngram in counts.traverse()}, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Probabilities & Querying Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30642504118616143\n",
      "0.36363636363636365\n",
      "0.0\n",
      "['italy', 'make'] 0.5\n",
      "['italy', 'in'] 0.5\n",
      "0.30642504118616143\n",
      "0.36363636363636365\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "raw_probs = ngramprobs(counts, logs=False)\n",
    "\n",
    "print(raw_probs.get(['of', 'the']).probability)\n",
    "print(raw_probs.get(['is', 'the']).probability)\n",
    "print(raw_probs.get(['the', 'play']).probability)  # not in training data\n",
    "\n",
    "for ngram in raw_probs.traverse(node=raw_probs.get(['italy']), sequence=['italy']):\n",
    "    print(ngram, raw_probs.get(ngram).probability)\n",
    "    \n",
    "# testing logp2p\n",
    "log_probs = ngramprobs(counts)\n",
    "\n",
    "print(logp2p(log_probs.get(['of', 'the']).probability))\n",
    "print(logp2p(log_probs.get(['is', 'the']).probability))\n",
    "print(logp2p(log_probs.get(['the', 'play']).probability))  # not in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $p(w_{1},\\ldots ,w_{n})$ ($p(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Probability of a Sequence: [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities (chain rule). \n",
    "\n",
    "$$p(w_{1}^{n}) = p(w_1) p(w_2|w_1) p(w_3|w_1^2) ... p(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{p(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$p(w_i|w_{1}^{i-1}) \\approx p(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\n",
    "N-gram   | Equation                   |\n",
    ":--------|:---------------------------|\n",
    "unigram  | $$p(w_i)$$                 |\n",
    "bigram   | $$p(w_i|w_{i-1})$$         |\n",
    "trigram  | $$p(w_i|w_{i-2},w_{i-1})$$ |\n",
    "\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$p(w_{1}^{n}) = \\prod_{i=1}^{n}{p(w_i|w_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Sentence beginning & end tags\n",
    "\n",
    "Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence tag, on the other hand, makes the bigram model a true probability distribution (Jurafsky and Martin). \"Without it, the sentence probabilities for all sentences of a given length would sum to one. This model would define an infinite set of probability distributions, with one distribution per sentence length.\"\n",
    "\n",
    "For larger ngrams, we’ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__: \"The cat is fat\"\n",
    "\n",
    "$$p(\\langle s \\rangle, the, cat, is, fat, \\langle /s \\rangle) =$$\n",
    "$$= p(the | \\langle s \\rangle) * p(cat | the) * p(is | cat) * p(fat | is) * p(\\langle /s \\rangle | fat) = $$\n",
    "$$= 0.25 * 0.10 * 0.20 * 0.05 * 0.15 = 0.0000375$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "- define a function to add sentence beginning and end tags to a corpus as list-of-lists\n",
    "- define a function to compute sentence probability given an ngram model\n",
    "    - remember that for log we use sum for raw probabilities product\n",
    "        - use `math.prod` for Python 3.8\n",
    "        - use `numpy.prod` for Python < 3.8\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU\n",
    "- compute probability of sentences: *star of twilight* and *star of thor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = \\langle s \\rangle$;\n",
    "- *while* $w_i \\neq \\langle /s \\rangle$\n",
    "\n",
    "    - stochastically get new word w.r.t. $p(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- define a function to generate random sentences (e.g. using `random.choice`)\n",
    "- generate `5` different sentences & score them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluation: [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\n",
    "$$ PPL = \\sqrt[N]{\\frac{1}{p(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}p(w_i|w_{i-N+1})}}$$\n",
    "\n",
    "Where $N$ is the number of words in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: blue;\">Solutions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_add_tags(corpus, bos='<s>', eos='</s>'):\n",
    "    \"\"\"\n",
    "    add beginning-of-sentence (bos) and end-of-sentence (eos) tags\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :param bos: beginning-of-sentence tag\n",
    "    :param eos: end-of-sentence tag\n",
    "    \"\"\"\n",
    "    return [[bos] + sent + [eos] for sent in corpus]\n",
    "\n",
    "def score(model, sentence):\n",
    "    \"\"\"\n",
    "    score a sentence given ngram model\n",
    "    :param model: trie ngram model\n",
    "    :param sentence: sentence as a list of tokens\n",
    "    :return: log probability\n",
    "    \"\"\"\n",
    "    from numpy import prod\n",
    "    probs = [model.get(ngram).probability for ngram in ngrams(sentence, model.size)]\n",
    "    return sum(probs) if model.logs else prod(probs)\n",
    "    \n",
    "def generate(model, bos='<s>', eos='</s>'):\n",
    "    \"\"\"\n",
    "    generate a random sequence from ngram model\n",
    "    :param model: trie ngram model\n",
    "    :param bos: beginning-of-sentence tag\n",
    "    :param eos: end-of-sentence tag\n",
    "    :return: sentence as list & log probability\n",
    "    \"\"\"\n",
    "    import random\n",
    "    word = bos\n",
    "    sent = [bos]\n",
    "    while word != eos:\n",
    "        c_node = model.get(sent[-(model.size - 1):])\n",
    "        word = random.choice(list(c_node.children.keys()))\n",
    "        sent.append(word)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.643604864128623\n",
      "-9.609769522510536\n"
     ]
    }
   ],
   "source": [
    "# tag corpus\n",
    "tagged_corpus = corpus_add_tags(corpus)\n",
    "\n",
    "# re-build ngram model\n",
    "tagged_counts = ngramcount(tagged_corpus, n=2)\n",
    "tagged_probs = ngramprobs(tagged_counts)\n",
    "\n",
    "# sentence adding tags\n",
    "sent1 = corpus_add_tags([['star', 'of', 'twilight']])[0]\n",
    "sent2 = corpus_add_tags([['star', 'of', 'thor']])[0]\n",
    "\n",
    "print(score(tagged_probs, sent1))\n",
    "print(score(tagged_probs, sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-10.60): <s> gross overall </s>\n",
      "(-8.11): <s> auqua fish </s>\n",
      "(-16.13): <s> which company for pulp fiction </s>\n",
      "(-10.95): <s> zombie apocolypse </s>\n",
      "(-25.03): <s> italian movies he was amazing </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    s = generate(tagged_probs)\n",
    "    print(\"({:5.2f}): {}\".format(score(tagged_probs, s), \" \".join(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Handling Unseen Words\n",
    "\n",
    "Out-Of-Vocabulary (OOV) word -- tokens in test data that are not contained in the lexicon (vocabulary).\n",
    "Empirically each OOV word results in 1.5 - 2 extra errors (> 1 due to the loss of contextual information).\n",
    "\n",
    "__*How to handle words (in test set) that were never seen in the training data?*__\n",
    "\n",
    "Train a language model with specific token (e.g. `<unk>`) for unknown words!\n",
    "\n",
    "__*How to estimate probabilities of unknown words and ngrams?*__\n",
    "\n",
    "The *simplest* approach is to replace all the words that are not in vocabulary (lexicon) with the `<unk>` token and treat it as any other word. (For instance, applying frequency cut-off to the lexicon, will allow estimate these probabilities on the training set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- define a function to replace OOV words in a corpus as list-of-list given a lexicon\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU (with `<unk>`)\n",
    "- re-compute probability of sentences: *star of twilight* and *star of thor*\n",
    "    - replace unknown words and add tags as well\n",
    "- compare scores to the ones without unknown word handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue;\">Solutions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_replace_oov(corpus, lexicon, unk='<unk>'):\n",
    "    \"\"\"\n",
    "    replace all tokens that are not in lexicon with unk\n",
    "    :param corpus: corpus as list-of-lists\n",
    "    :param lexicon: lexicon as a list of tokens\n",
    "    :return: processed corpus\n",
    "    \"\"\"\n",
    "    return [[token if token in lexicon else unk for token in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing OOV & re-scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.643604864128623\n",
      "-13.731498255275248\n"
     ]
    }
   ],
   "source": [
    "# replace unknown corpus\n",
    "unk_lexicon = cutoff(compute_frequency_list(tagged_corpus), tf_min=2)\n",
    "unk_corpus = corpus_replace_oov(tagged_corpus, unk_lexicon)\n",
    "\n",
    "# re-build ngram model\n",
    "unk_counts = ngramcount(unk_corpus, n=2)\n",
    "unk_probs = ngramprobs(unk_counts)\n",
    "\n",
    "# sentence adding tags\n",
    "sent1 = corpus_replace_oov(corpus_add_tags([['star', 'of', 'twilight']]), unk_lexicon)[0]\n",
    "sent2 = corpus_replace_oov(corpus_add_tags([['star', 'of', 'thor']]), unk_lexicon)[0]\n",
    "\n",
    "print(score(unk_probs, sent1))\n",
    "print(score(unk_probs, sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Handling Data Sparseness\n",
    "\n",
    "What do we do with words that are in our lexicon, but appear in a test set in an unseen context (i.e. no ngram)?\n",
    "\n",
    "Similar to unseen words and unseen n-grams have $0$ probability; thus, whole sequence gets $0$ probability.\n",
    "(The problem is somewhat avoided using log probabilities.)\n",
    "\n",
    "Use smoothing:\n",
    "- Add some probability to unseen events\n",
    "- Remove some probability from seen events\n",
    "- Joint probability distribution sums to 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smoothing Methods\n",
    "\n",
    "Available smoothing methods: ([tutorial](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf))\n",
    "- [Additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (__simplest__)\n",
    "- Good-Turing estimate\n",
    "- Jelinek-Mercer smoothing (interpolation)\n",
    "- Katz smoothing (backoff)\n",
    "- Witten-Bell smoothing\n",
    "- Absolute discounting\n",
    "- Kneser-Ney smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1. Add-One Smoothing\n",
    "Kind of Additive Smoothing.\n",
    "\n",
    "__Bigrams__\n",
    "\n",
    "$V$ -- vocabulary size\n",
    "\n",
    "$$p(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$\n",
    "\n",
    "__N-grams__\n",
    "\n",
    "$V$ -- total number of possible $(N-1)$-grams\n",
    "\n",
    "$$p(w_i | w^{i-1}_{i-N+1}) = \\frac{c(w^{i-1}_{i-N+1},w_i)+1}{c(w^{i-1}_{i-N+1})+V}$$\n",
    "\n",
    "Typically, we assume $V = \\{w : c(w) > 0\\} \\cup \\{<unk>\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- update the ngram probability calculation function to apply add one smoothing\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU with smoothing\n",
    "- compare scores to the ones without smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue;\">Solutions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_vocabulary_size(counts, n=1):\n",
    "    return len(set([\"+\".join(ngram[:n]) for ngram in counts.traverse()]))\n",
    "\n",
    "def ngramprobs(counts, logs=True, smoothing=False):\n",
    "    \"\"\"\n",
    "    compute ngram probabilities from frequency counts\n",
    "    :param counts: counts trie\n",
    "    :param logs: if to compute log-probabilities\n",
    "    :param smoothing: if to use add 1 smoothing\n",
    "    :return: trie augmented with probabilties\n",
    "    \"\"\"\n",
    "    from math import log\n",
    "    \n",
    "    # set meta-information\n",
    "    counts.logs = logs  # meta-info: log probabilities are used\n",
    "    counts.smoothing = smoothing # meta-info: smoothing true|false\n",
    "    \n",
    "    # add 1 smoothing\n",
    "    v = compute_ngram_vocabulary_size(counts, n = counts.size - 1) if smoothing else 0\n",
    "    a = 1 if smoothing else 0\n",
    "    \n",
    "    # update error probability:\n",
    "    counts.error.probability = log(a / v) if (smoothing and logs) else 0.0\n",
    "    \n",
    "    for ngram in counts.traverse(): \n",
    "        n = counts.get(ngram)       # get ngram node\n",
    "        p = counts.get(ngram[:-1])  # get parent node\n",
    "        prob = (n.count + a) / (p.count + v)\n",
    "        n.probability = log(prob) if logs else prob\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23.038581629648416\n",
      "-17.936125948586973\n"
     ]
    }
   ],
   "source": [
    "# re-build ngram model\n",
    "sm_probs = ngramprobs(unk_counts, smoothing=True)\n",
    "\n",
    "# sentence adding tags\n",
    "sent1 = corpus_replace_oov(corpus_add_tags([['star', 'of', 'twilight']]), unk_lexicon)[0]\n",
    "sent2 = corpus_replace_oov(corpus_add_tags([['star', 'of', 'thor']]), unk_lexicon)[0]\n",
    "\n",
    "print(score(unk_probs, sent1))\n",
    "print(score(unk_probs, sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Putting it all together (Exercise)\n",
    "\n",
    "Train an Ngram Language Model (compute ngram probabilities) such that:\n",
    "\n",
    "- case insensitive (by default)\n",
    "- 2-gram\n",
    "- log probabilities\n",
    "- considers sentence boundaries (beginning and end of sentence tags)\n",
    "- considers unknown words\n",
    "- Add-One Smoothing\n",
    "\n",
    "Compute probabilties of utterances in `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue;\">Solution</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-17.94): <s> star of <unk> </s>\n",
      "(-27.43): <s> who is in the movie the campaign </s>\n",
      "(-34.34): <s> list the cast of the movie the campaign </s>\n",
      "(-21.80): <s> who was in twilight </s>\n",
      "(-16.45): <s> who is in <unk> </s>\n",
      "(-24.49): <s> actor from lost </s>\n",
      "(-29.63): <s> who played in the movie rocky </s>\n",
      "(-32.10): <s> who played in the movie captain america </s>\n",
      "(-29.33): <s> cast and crew for in july </s>\n",
      "(-27.05): <s> who is in movie in july </s>\n"
     ]
    }
   ],
   "source": [
    "trn='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
    "tst='NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'\n",
    "\n",
    "trn_data_raw = read_corpus(trn)\n",
    "trn_data_tag = corpus_add_tags(trn_data_raw)\n",
    "\n",
    "# make lexicon\n",
    "lex = cutoff(compute_frequency_list(tagged_corpus), tf_min=2)\n",
    "\n",
    "trn_data_unk = corpus_replace_oov(trn_data_tag, lex)\n",
    "\n",
    "# re-build ngram model\n",
    "counts = ngramcount(trn_data_unk, n=2)\n",
    "probs = ngramprobs(counts, logs=True, smoothing=True)\n",
    "\n",
    "# prepare test set\n",
    "tst_data_raw = read_corpus(tst)\n",
    "tst_data_tag = corpus_add_tags(tst_data_raw)\n",
    "tst_data_unk = corpus_replace_oov(tst_data_tag, lex)\n",
    "\n",
    "for sent in tst_data_unk[:10]:\n",
    "    print(\"({:5.2f}): {}\".format(score(probs, sent), \" \".join(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
